#!/usr/bin/env python3
"""
ä¸€ä½“åŒ–æ™ºèƒ½äº¤æ˜“è„šæœ¬
å®Œæˆï¼šæ•°æ®ä¸‹è½½/ç¼“å­˜ â†’ ç‰¹å¾å·¥ç¨‹ â†’ æ™ºèƒ½é€‰è‚¡ï¼ˆMLï¼‰ â†’ ç­–ç•¥å›æµ‹ â†’ æŠ¥å‘Šç”Ÿæˆ
"""

import sys
import argparse
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_root))

from tradingagents.dataflows.data_downloader import DataDownloader
from tradingagents.models.ml_features import extract_features, select_features
from tradingagents.models.ml_selector import SmartSelector
from tradingagents.backtest.backtest_strategy import create_strategy, MAStrategy
from tradingagents.backtest.engine import BacktestEngine
from tradingagents.backtest.backtest_report import BacktestReport
from tradingagents.utils.logging_init import get_logger

logger = get_logger('scripts.smart_trading')


def run_smart_trading(
    symbols: list,
    start_date: str,
    end_date: str,
    provider: str = "tushare",
    data_dir: str = "data",
    run_dir: str = "runs",
    train_ml: bool = True,
    strategy_type: str = "ma"
):
    """
    è¿è¡Œä¸€ä½“åŒ–æ™ºèƒ½äº¤æ˜“æµç¨‹
    
    Args:
        symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆå¦‚ ["600519.SH", "000001.SZ"]ï¼‰
        start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
        provider: æ•°æ®æä¾›å•†
        data_dir: æ•°æ®ç›®å½•
        run_dir: è¿è¡Œç»“æœç›®å½•
        train_ml: æ˜¯å¦è®­ç»ƒMLæ¨¡å‹
        strategy_type: å›æµ‹ç­–ç•¥ç±»å‹ï¼ˆma/momentum/mlï¼‰
    """
    # åˆ›å»ºè¿è¡Œç›®å½•
    run_id = datetime.now().strftime('%Y%m%d_%H%M%S')
    run_path = Path(run_dir) / run_id
    run_path.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"ğŸš€ å¼€å§‹æ™ºèƒ½äº¤æ˜“æµç¨‹ (run_id={run_id})")
    
    # === æ­¥éª¤1: æ•°æ®ä¸‹è½½ ===
    logger.info("ğŸ“¥ æ­¥éª¤1: ä¸‹è½½è‚¡ç¥¨æ•°æ®...")
    downloader = DataDownloader(
        save_path=f"{data_dir}/stock_daily.parquet",
        provider=provider
    )
    
    all_data = {}
    for symbol in symbols:
        try:
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            start_ts = pd.to_datetime(start_date).strftime('%Y%m%d')
            end_ts = pd.to_datetime(end_date).strftime('%Y%m%d')
            
            df = downloader.get_stock_data(symbol, start_ts, end_ts)
            if not df.empty:
                all_data[symbol] = df
                logger.info(f"âœ… {symbol}: {len(df)} æ¡è®°å½•")
            else:
                logger.warning(f"âš ï¸ {symbol}: æœªè·å–åˆ°æ•°æ®")
        except Exception as e:
            logger.error(f"âŒ {symbol} ä¸‹è½½å¤±è´¥: {e}")
    
    if not all_data:
        logger.error("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®ï¼Œé€€å‡º")
        return
    
    # === æ­¥éª¤2: ç‰¹å¾å·¥ç¨‹ ===
    logger.info("ğŸ”§ æ­¥éª¤2: ç‰¹å¾å·¥ç¨‹...")
    all_features = {}
    
    for symbol, df in all_data.items():
        try:
            # ç¡®ä¿åˆ—åæ­£ç¡®
            if 'trade_date' in df.columns:
                df = df.set_index('trade_date')
            
            features_df = extract_features(df)
            if not features_df.empty:
                all_features[symbol] = features_df
                
                # ä¿å­˜ç‰¹å¾
                feature_file = run_path / f"{symbol.replace('.', '_')}_features.parquet"
                try:
                    features_df.to_parquet(feature_file)
                except:
                    features_df.to_csv(str(feature_file).replace('.parquet', '.csv'), index=False)
                
                logger.info(f"âœ… {symbol}: {len(features_df.columns)} ä¸ªç‰¹å¾")
        except Exception as e:
            logger.error(f"âŒ {symbol} ç‰¹å¾æå–å¤±è´¥: {e}")
    
    if not all_features:
        logger.error("âŒ æœªæå–åˆ°ä»»ä½•ç‰¹å¾ï¼Œé€€å‡º")
        return
    
    # === æ­¥éª¤3: æ™ºèƒ½é€‰è‚¡ï¼ˆMLï¼‰===
    ml_scores = {}
    ml_model = None
    
    if train_ml:
        logger.info("ğŸ§  æ­¥éª¤3: è®­ç»ƒMLæ¨¡å‹å¹¶è¿›è¡Œé€‰è‚¡...")
        
        # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨çš„ç‰¹å¾ç”¨äºè®­ç»ƒ
        train_features_list = []
        train_labels_list = []
        
        for symbol, features_df in all_features.items():
            # é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆæ’é™¤ç›®æ ‡åˆ—ï¼‰
            feature_cols = select_features(features_df)
            
            if not feature_cols.empty and 'future_return_binary' in features_df.columns:
                # ç§»é™¤NaNè¡Œ
                valid_mask = ~(feature_cols.isna().any(axis=1) | features_df['future_return_binary'].isna())
                
                train_features_list.append(feature_cols[valid_mask])
                train_labels_list.append(features_df.loc[valid_mask, 'future_return_binary'])
        
        if train_features_list:
            combined_features = pd.concat(train_features_list, ignore_index=True)
            combined_labels = pd.concat(train_labels_list, ignore_index=True)
            
            # è®­ç»ƒæ¨¡å‹
            model_path = run_path / "ml_model.pkl"
            ml_model = SmartSelector(model_type="classifier", model_path=str(model_path))
            
            metrics = ml_model.train(combined_features, combined_labels)
            logger.info(f"âœ… MLæ¨¡å‹è®­ç»ƒå®Œæˆ: {metrics}")
            
            # å¯¹æ¯åªè‚¡ç¥¨è¿›è¡Œé¢„æµ‹
            for symbol, features_df in all_features.items():
                feature_cols = select_features(features_df)
                if not feature_cols.empty:
                    predictions = ml_model.predict_stocks(feature_cols, return_proba=True)
                    if 'probability' in predictions.columns:
                        ml_scores[symbol] = predictions['probability'].iloc[-1]  # æœ€æ–°é¢„æµ‹
                        logger.info(f"âœ… {symbol}: MLè¯„åˆ† = {ml_scores[symbol]:.4f}")
    else:
        logger.info("â­ï¸ æ­¥éª¤3: è·³è¿‡MLè®­ç»ƒï¼Œä½¿ç”¨ç®€å•è¯„åˆ†...")
        # ç®€å•è¯„åˆ†ï¼šä½¿ç”¨åŠ¨é‡
        for symbol, features_df in all_features.items():
            if 'momentum_20' in features_df.columns:
                ml_scores[symbol] = features_df['momentum_20'].iloc[-1]
    
    # ä¿å­˜é€‰è‚¡ç»“æœ
    if ml_scores:
        scores_df = pd.DataFrame([
            {'symbol': k, 'ml_score': v}
            for k, v in ml_scores.items()
        ]).sort_values('ml_score', ascending=False)
        
        scores_file = run_path / "scores_today.csv"
        scores_df.to_csv(scores_file, index=False)
        logger.info(f"âœ… é€‰è‚¡ç»“æœå·²ä¿å­˜: {scores_file}")
    
    # === æ­¥éª¤4: ç­–ç•¥å›æµ‹ ===
    logger.info("ğŸ“Š æ­¥éª¤4: è¿è¡Œå›æµ‹...")
    
    backtest_results = {}
    
    for symbol, df in all_data.items():
        try:
            # åˆ›å»ºç­–ç•¥
            if strategy_type == "ma":
                strategy = MAStrategy(fast_period=5, slow_period=20)
            elif strategy_type == "ml" and ml_model:
                strategy = create_strategy("ml", model=ml_model, threshold=0.5)
            else:
                strategy = MAStrategy()  # é»˜è®¤
            
            # ç”Ÿæˆä¿¡å·
            signals_df = strategy.generate_signals(df)
            
            if signals_df.empty or 'signal' not in signals_df.columns:
                logger.warning(f"âš ï¸ {symbol}: æœªç”Ÿæˆä¿¡å·")
                continue
            
            # åˆå¹¶æ•°æ®å’Œä¿¡å·
            df_with_signals = df.copy()
            if 'trade_date' in df_with_signals.columns:
                df_with_signals = df_with_signals.set_index('trade_date')
            
            df_with_signals['signal'] = signals_df['signal'].reindex(df_with_signals.index, fill_value=0)
            
            # è¿è¡Œå›æµ‹
            engine = BacktestEngine(
                data=df_with_signals,
                strategies=[strategy_type],
                initial_capital=100000.0,
                commission_rate=0.0003,
                slippage_rate=0.0001
            )
            
            # è¦†ç›–ä¿¡å·ç”Ÿæˆæ–¹æ³•
            def _calculate_signals(data):
                signals = []
                for idx, row in data.iterrows():
                    signal_val = row.get('signal', 0)
                    if signal_val == 1:
                        signals.append({
                            'side': 'BUY',
                            'price': float(row.get('close', 0)),
                            'qty': 100,
                            'row': row.to_dict()
                        })
                    elif signal_val == -1:
                        signals.append({
                            'side': 'SELL',
                            'price': float(row.get('close', 0)),
                            'qty': 100,
                            'row': row.to_dict()
                        })
                return signals
            
            engine.calculate_signals = _calculate_signals
            engine.execute()
            
            # ç”ŸæˆæŠ¥å‘Š
            report = BacktestReport(engine)
            backtest_results[symbol] = report.generate_report()
            
            logger.info(f"âœ… {symbol}: å›æµ‹å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ {symbol} å›æµ‹å¤±è´¥: {e}")
    
    # ä¿å­˜æ‰€æœ‰å›æµ‹ç»“æœ
    if backtest_results:
        all_scores = []
        for symbol, result in backtest_results.items():
            summary = result.get('summary', {})
            metrics = result.get('metrics', {})
            all_scores.append({
                'symbol': symbol,
                'total_return': summary.get('total_return', 0),
                'annualized_return': metrics.get('annualized_return', 0),
                'sharpe_ratio': metrics.get('sharpe_ratio', 0),
                'max_drawdown': metrics.get('max_drawdown', 0),
                'ml_score': ml_scores.get(symbol, 0)
            })
        
        all_scores_df = pd.DataFrame(all_scores)
        all_scores_file = run_path / "scores_all.parquet"
        try:
            all_scores_df.to_parquet(all_scores_file)
        except:
            all_scores_df.to_csv(str(all_scores_file).replace('.parquet', '.csv'), index=False)
        
        logger.info(f"âœ… æ‰€æœ‰å›æµ‹ç»“æœå·²ä¿å­˜: {all_scores_file}")
    
    # === ä¿å­˜å…ƒæ•°æ® ===
    meta = {
        'run_id': run_id,
        'symbols': symbols,
        'start_date': start_date,
        'end_date': end_date,
        'provider': provider,
        'strategy_type': strategy_type,
        'train_ml': train_ml,
        'created_at': datetime.now().isoformat()
    }
    
    meta_file = run_path / "meta.json"
    import json
    with open(meta_file, 'w', encoding='utf-8') as f:
        json.dump(meta, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ… æµç¨‹å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {run_path}")
    print(f"\nğŸ“ è¿è¡Œç»“æœ: {run_path}")
    print(f"   - ç‰¹å¾æ–‡ä»¶: features.parquet")
    print(f"   - é€‰è‚¡ç»“æœ: scores_today.csv")
    print(f"   - å›æµ‹ç»“æœ: scores_all.csv")
    print(f"   - å…ƒæ•°æ®: meta.json\n")


def main():
    parser = argparse.ArgumentParser(description='ä¸€ä½“åŒ–æ™ºèƒ½äº¤æ˜“è„šæœ¬')
    parser.add_argument('--symbols', nargs='+', required=True, help='è‚¡ç¥¨ä»£ç åˆ—è¡¨')
    parser.add_argument('--start', required=True, help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end', required=True, help='ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--provider', default='tushare', choices=['tushare', 'akshare'], help='æ•°æ®æä¾›å•†')
    parser.add_argument('--data-dir', default='data', help='æ•°æ®ç›®å½•')
    parser.add_argument('--run-dir', default='runs', help='è¿è¡Œç»“æœç›®å½•')
    parser.add_argument('--no-train-ml', action='store_true', help='ä¸è®­ç»ƒMLæ¨¡å‹')
    parser.add_argument('--strategy', default='ma', choices=['ma', 'momentum', 'ml'], help='å›æµ‹ç­–ç•¥ç±»å‹')
    
    args = parser.parse_args()
    
    run_smart_trading(
        symbols=args.symbols,
        start_date=args.start,
        end_date=args.end,
        provider=args.provider,
        data_dir=args.data_dir,
        run_dir=args.run_dir,
        train_ml=not args.no_train_ml,
        strategy_type=args.strategy
    )


if __name__ == "__main__":
    main()

