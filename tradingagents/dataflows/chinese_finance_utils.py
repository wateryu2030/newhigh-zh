#!/usr/bin/env python3
"""
ä¸­å›½è´¢ç»æ•°æ®èšåˆå·¥å…·
ç”±äºå¾®åšAPIç”³è¯·å›°éš¾ä¸”åŠŸèƒ½å—é™ï¼Œé‡‡ç”¨å¤šæºæ•°æ®èšåˆçš„æ–¹å¼
"""

import requests
import json
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import re
from bs4 import BeautifulSoup
import pandas as pd

# å¯¼å…¥æ—¥å¿—ç³»ç»Ÿ
try:
    from tradingagents.utils.logging_init import get_logger
    logger = get_logger('dataflows.chinese_finance')
except:
    import logging
    logger = logging.getLogger('chinese_finance')


class ChineseFinanceDataAggregator:
    """ä¸­å›½è´¢ç»æ•°æ®èšåˆå™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def get_stock_sentiment_summary(self, ticker: str, days: int = 7) -> Dict:
        """
        è·å–è‚¡ç¥¨æƒ…ç»ªåˆ†ææ±‡æ€»
        æ•´åˆå¤šä¸ªå¯è·å–çš„ä¸­å›½è´¢ç»æ•°æ®æº
        """
        try:
            # 1. è·å–è´¢ç»æ–°é—»æƒ…ç»ª
            news_sentiment = self._get_finance_news_sentiment(ticker, days)
            
            # 2. è·å–è‚¡å§è®¨è®ºçƒ­åº¦ (å¦‚æœå¯ä»¥è·å–)
            forum_sentiment = self._get_stock_forum_sentiment(ticker, days)
            
            # 3. è·å–è´¢ç»åª’ä½“æŠ¥é“
            media_sentiment = self._get_media_coverage_sentiment(ticker, days)
            
            # 4. ç»¼åˆåˆ†æ
            overall_sentiment = self._calculate_overall_sentiment(
                news_sentiment, forum_sentiment, media_sentiment
            )
            
            return {
                'ticker': ticker,
                'analysis_period': f'{days} days',
                'overall_sentiment': overall_sentiment,
                'news_sentiment': news_sentiment,
                'forum_sentiment': forum_sentiment,
                'media_sentiment': media_sentiment,
                'summary': self._generate_sentiment_summary(overall_sentiment),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                'ticker': ticker,
                'error': f'æ•°æ®è·å–å¤±è´¥: {str(e)}',
                'fallback_message': 'ç”±äºä¸­å›½ç¤¾äº¤åª’ä½“APIé™åˆ¶ï¼Œå»ºè®®ä½¿ç”¨è´¢ç»æ–°é—»å’ŒåŸºæœ¬é¢åˆ†æä½œä¸ºä¸»è¦å‚è€ƒ',
                'timestamp': datetime.now().isoformat()
            }
    
    def _get_finance_news_sentiment(self, ticker: str, days: int) -> Dict:
        """è·å–è´¢ç»æ–°é—»æƒ…ç»ªåˆ†æ"""
        try:
            # æœç´¢ç›¸å…³æ–°é—»æ ‡é¢˜å’Œå†…å®¹
            company_name = self._get_company_chinese_name(ticker)
            search_terms = [ticker, company_name] if company_name else [ticker]
            
            news_items = []
            for term in search_terms:
                # è¿™é‡Œå¯ä»¥é›†æˆå¤šä¸ªæ–°é—»æº
                items = self._search_finance_news(term, days)
                news_items.extend(items)
            
            # ç®€å•çš„æƒ…ç»ªåˆ†æ
            positive_count = 0
            negative_count = 0
            neutral_count = 0
            
            for item in news_items:
                sentiment = self._analyze_text_sentiment(item.get('title', '') + ' ' + item.get('content', ''))
                if sentiment > 0.1:
                    positive_count += 1
                elif sentiment < -0.1:
                    negative_count += 1
                else:
                    neutral_count += 1
            
            total = len(news_items)
            if total == 0:
                return {'sentiment_score': 0, 'confidence': 0, 'news_count': 0}
            
            sentiment_score = (positive_count - negative_count) / total
            
            return {
                'sentiment_score': sentiment_score,
                'positive_ratio': positive_count / total,
                'negative_ratio': negative_count / total,
                'neutral_ratio': neutral_count / total,
                'news_count': total,
                'confidence': min(total / 10, 1.0)  # æ–°é—»æ•°é‡è¶Šå¤šï¼Œç½®ä¿¡åº¦è¶Šé«˜
            }
            
        except Exception as e:
            return {'error': str(e), 'sentiment_score': 0, 'confidence': 0}
    
    def _get_stock_forum_sentiment(self, ticker: str, days: int) -> Dict:
        """è·å–è‚¡ç¥¨è®ºå›è®¨è®ºæƒ…ç»ª - ä»ä¸œæ–¹è´¢å¯Œè‚¡å§å’Œé›ªçƒè·å–çœŸå®æ•°æ®"""
        try:
            # åˆ¤æ–­æ˜¯å¦ä¸ºAè‚¡ä»£ç ï¼ˆ6ä½æ•°å­—ï¼‰
            if not re.match(r'^\d{6}$', ticker):
                # éAè‚¡ï¼Œè¿”å›ç©ºæ•°æ®
                return {
                    'sentiment_score': 0,
                    'discussion_count': 0,
                    'hot_topics': [],
                    'note': f'è‚¡ç¥¨ä»£ç  {ticker} éAè‚¡æ ¼å¼ï¼Œè‚¡å§æ•°æ®ä»…æ”¯æŒAè‚¡',
                    'confidence': 0
                }
            
            # è·å–ä¸œæ–¹è´¢å¯Œè‚¡å§æ•°æ®
            forum_data = self._fetch_eastmoney_guba(ticker, days)
            
            # å°è¯•è·å–é›ªçƒæ•°æ®ä½œä¸ºè¡¥å……
            xueqiu_data = self._fetch_xueqiu_discussion(ticker, days)
            
            # åˆå¹¶å¤šä¸ªå¹³å°çš„æ•°æ®
            if xueqiu_data.get('discussion_count', 0) > 0:
                # åˆå¹¶ä¸¤ä¸ªå¹³å°çš„æ•°æ®
                all_discussions = forum_data.get('discussions', []) + xueqiu_data.get('discussions', [])
                all_hot_topics = forum_data.get('hot_topics', []) + xueqiu_data.get('hot_topics', [])
                
                # é‡æ–°è®¡ç®—ç»¼åˆæƒ…ç»ª
                if all_discussions:
                    sentiment_scores = []
                    for discussion in all_discussions:
                        content = discussion.get('title', '') + ' ' + discussion.get('content', '')
                        sentiment = self._analyze_text_sentiment(content)
                        sentiment_scores.append(sentiment)
                    
                    avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0
                    
                    forum_data = {
                        'discussions': all_discussions,
                        'discussion_count': len(all_discussions),
                        'hot_topics': list(set(all_hot_topics))[:15],
                        'source': 'ä¸œæ–¹è´¢å¯Œè‚¡å§ + é›ªçƒ',
                        'platform': 'ä¸œæ–¹è´¢å¯Œè‚¡å§ + é›ªçƒ',
                        'sentiment_score': avg_sentiment
                    }
            
            if not forum_data or forum_data.get('discussion_count', 0) == 0:
                return {
                    'sentiment_score': 0,
                    'discussion_count': 0,
                    'hot_topics': [],
                    'note': 'æœªè·å–åˆ°è‚¡å§è®¨è®ºæ•°æ®',
                    'confidence': 0
                }
            
            # åˆ†æè®¨è®ºæƒ…ç»ª
            discussions = forum_data.get('discussions', [])
            sentiment_scores = []
            for discussion in discussions:
                content = discussion.get('title', '') + ' ' + discussion.get('content', '')
                sentiment = self._analyze_text_sentiment(content)
                sentiment_scores.append(sentiment)
            
            avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0
            
            return {
                'sentiment_score': avg_sentiment,
                'discussion_count': forum_data.get('discussion_count', 0),
                'hot_topics': forum_data.get('hot_topics', [])[:10],  # å‰10ä¸ªçƒ­é—¨è¯é¢˜
                'platform': 'ä¸œæ–¹è´¢å¯Œè‚¡å§',
                'confidence': min(forum_data.get('discussion_count', 0) / 20, 1.0)  # è®¨è®ºè¶Šå¤šç½®ä¿¡åº¦è¶Šé«˜
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–è‚¡å§æ•°æ®å¤±è´¥: {e}")
            return {
                'sentiment_score': 0,
                'discussion_count': 0,
                'hot_topics': [],
                'note': f'è‚¡å§æ•°æ®è·å–å¤±è´¥: {str(e)}',
                'confidence': 0
            }
    
    def _fetch_eastmoney_guba(self, ticker: str, days: int) -> Dict:
        """ä»ä¸œæ–¹è´¢å¯Œè‚¡å§è·å–è‚¡ç¥¨è®¨è®ºæ•°æ®"""
        try:
            base_url = "https://guba.eastmoney.com"
            discussions = []
            hot_topics = []
            
            # æ–¹æ³•1: ä½¿ç”¨ä¸œæ–¹è´¢å¯Œè‚¡å§çš„JSON APIæ¥å£
            try:
                # ä¸œæ–¹è´¢å¯Œè‚¡å§å®é™…ä½¿ç”¨çš„APIæ¥å£
                # APIæ ¼å¼: https://guba.eastmoney.com/list,{è‚¡ç¥¨ä»£ç },f_{é¡µç }.html
                # ä½†æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨JSON APIè·å–æ•°æ®
                
                # å°è¯•è·å–æœ€æ–°çš„å¸–å­åˆ—è¡¨ï¼ˆç¬¬1é¡µï¼‰
                api_url = f"https://guba.eastmoney.com/list,{ticker},f_1.html"
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Referer': f'https://quote.eastmoney.com/{ticker}.html',
                    'Connection': 'keep-alive',
                }
                
                response = self.session.get(api_url, headers=headers, timeout=15)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ä¸œæ–¹è´¢å¯Œè‚¡å§çš„å®é™…HTMLç»“æ„
                    # å¸–å­é€šå¸¸åœ¨ <div class="articleh"> æˆ–ç±»ä¼¼çš„å®¹å™¨ä¸­
                    post_containers = soup.find_all(['div', 'td'], class_=re.compile(r'articleh|title', re.I))
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æ›´é€šç”¨çš„é€‰æ‹©å™¨
                    if not post_containers:
                        # æŸ¥æ‰¾åŒ…å«å¸–å­æ ‡é¢˜çš„é“¾æ¥
                        post_links = soup.find_all('a', href=re.compile(r'/news,.*\.html'))
                        post_containers = post_links
                    
                    # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ–‡æœ¬çš„é“¾æ¥
                    if not post_containers:
                        all_links = soup.find_all('a', href=True)
                        # è¿‡æ»¤å‡ºå¯èƒ½æ˜¯å¸–å­é“¾æ¥çš„
                        post_containers = [link for link in all_links 
                                          if any(keyword in link.get_text() for keyword in ['è®¨è®º', 'åˆ†æ', 'å…¬å‘Š', 'ä¸šç»©']) 
                                          or len(link.get_text().strip()) > 10]
                    
                    count = 0
                    for container in post_containers[:30]:  # æœ€å¤š30ä¸ªå¸–å­
                        try:
                            # æå–æ ‡é¢˜
                            if hasattr(container, 'get_text'):
                                title = container.get_text(strip=True)
                            elif hasattr(container, 'text'):
                                title = container.text.strip()
                            else:
                                title = str(container).strip()
                            
                            # æå–é“¾æ¥
                            href = ''
                            if hasattr(container, 'get'):
                                href = container.get('href', '')
                            elif hasattr(container, 'find') and container.find('a'):
                                href = container.find('a').get('href', '')
                            
                            # è¿‡æ»¤æœ‰æ•ˆæ ‡é¢˜
                            if title and len(title) > 5 and len(title) < 200:
                                discussions.append({
                                    'title': title,
                                    'content': title,  # åˆ—è¡¨é¡µé€šå¸¸åªæœ‰æ ‡é¢˜
                                    'url': f"{base_url}{href}" if href and not href.startswith('http') else (href if href.startswith('http') else ''),
                                    'source': 'ä¸œæ–¹è´¢å¯Œè‚¡å§'
                                })
                                count += 1
                                
                                # æå–çƒ­é—¨è¯é¢˜å…³é”®è¯
                                if any(keyword in title for keyword in ['æ¶¨åœ', 'è·Œåœ', 'åˆ©å¥½', 'åˆ©ç©º', 'å…¬å‘Š', 'ä¸šç»©', 'çªç ´', 'å›è°ƒ']):
                                    hot_topics.append(title[:50])  # é™åˆ¶é•¿åº¦
                        except Exception as e:
                            logger.debug(f"è§£æå•ä¸ªå¸–å­å¤±è´¥: {e}")
                            continue
                    
                    if discussions:
                        logger.info(f"âœ… ä»ä¸œæ–¹è´¢å¯Œè‚¡å§è·å–åˆ° {len(discussions)} æ¡è®¨è®º")
                        return {
                            'discussions': discussions,
                            'discussion_count': len(discussions),
                            'hot_topics': list(set(hot_topics))[:15],  # å»é‡å¹¶é™åˆ¶æ•°é‡
                            'source': 'ä¸œæ–¹è´¢å¯Œè‚¡å§'
                        }
                    else:
                        logger.warning(f"âš ï¸ è‚¡å§é¡µé¢è§£ææˆåŠŸä½†æœªæ‰¾åˆ°æœ‰æ•ˆå¸–å­")
                
            except Exception as e:
                logger.debug(f"è‚¡å§APIè®¿é—®å¤±è´¥: {e}")
            
            # æ–¹æ³•2: å°è¯•ä½¿ç”¨AKShareè·å–è‚¡ç¥¨è®¨è®ºç›¸å…³çš„å…¶ä»–ä¿¡æ¯
            try:
                import akshare as ak
                # AKShareè™½ç„¶æ²¡æœ‰ç›´æ¥çš„è‚¡å§APIï¼Œä½†å¯ä»¥è·å–è‚¡ç¥¨å…¬å‘Šç­‰ä¿¡æ¯
                # è¿™äº›ä¹Ÿå¯ä»¥åæ˜ å¸‚åœºè®¨è®ºçƒ­ç‚¹
                announcements = self._get_stock_announcements_akshare(ticker)
                if announcements:
                    return {
                        'discussions': announcements,
                        'discussion_count': len(announcements),
                        'hot_topics': [a.get('title', '') for a in announcements[:10]],
                        'source': 'AKShareå…¬å‘Šæ•°æ®ï¼ˆè¡¥å……ï¼‰'
                    }
            except ImportError:
                pass
            except Exception as e:
                logger.debug(f"AKShareè¡¥å……æ•°æ®è·å–å¤±è´¥: {e}")
            
            # å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›è¯´æ˜
            return {
                'discussions': [],
                'discussion_count': 0,
                'hot_topics': [],
                'note': f'æ— æ³•ç›´æ¥è·å–è‚¡å§æ•°æ®ï¼Œå¯æ‰‹åŠ¨è®¿é—®: https://guba.eastmoney.com/list,{ticker}.html',
                'source': 'ä¸œæ–¹è´¢å¯Œè‚¡å§ï¼ˆéœ€è¦æ‰‹åŠ¨è®¿é—®ï¼‰'
            }
            
        except Exception as e:
            logger.warning(f"è·å–è‚¡å§æ•°æ®å¼‚å¸¸: {e}")
            return {
                'discussions': [],
                'discussion_count': 0,
                'hot_topics': [],
                'error': str(e),
                'source': 'ä¸œæ–¹è´¢å¯Œè‚¡å§'
            }
    
    def _get_stock_announcements_akshare(self, ticker: str) -> List[Dict]:
        """ä½¿ç”¨AKShareè·å–è‚¡ç¥¨å…¬å‘Šï¼ˆå¯ä½œä¸ºè®¨è®ºçƒ­ç‚¹ï¼‰"""
        try:
            import akshare as ak
            # è·å–è‚¡ç¥¨å…¬å‘Šï¼ˆè¿™äº›å¾€å¾€ä¼šå¼•èµ·è®¨è®ºï¼‰
            # æ³¨æ„ï¼šéœ€è¦æ ¹æ®AKShareçš„å®é™…APIè°ƒæ•´
            announcements = []
            
            # å°è¯•è·å–å…¬å‘Šæ•°æ®
            try:
                # AKShareçš„å…¬å‘Šæ¥å£ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®AKShareçš„å®é™…APIæ–‡æ¡£è°ƒæ•´
                announcements_data = ak.stock_notice_report(stock=ticker, indicator="å…¬å‘Š")
                if announcements_data is not None and not announcements_data.empty:
                    for _, row in announcements_data.head(10).iterrows():
                        title = str(row.get('å…¬å‘Šæ ‡é¢˜', row.get('title', ''))).strip()
                        if title:
                            announcements.append({
                                'title': title,
                                'content': str(row.get('å…¬å‘Šå†…å®¹', '')),
                                'source': 'AKShareå…¬å‘Š',
                                'url': ''
                            })
            except:
                pass
            
            return announcements
        except:
            return []
    
    def _fetch_xueqiu_discussion(self, ticker: str, days: int) -> Dict:
        """ä»é›ªçƒå¹³å°è·å–è‚¡ç¥¨è®¨è®ºæ•°æ®"""
        try:
            discussions = []
            hot_topics = []
            
            # é›ªçƒå¹³å°çš„APIæˆ–é¡µé¢è®¿é—®
            # æ³¨æ„ï¼šé›ªçƒä¹Ÿæœ‰åçˆ¬è™«æœºåˆ¶ï¼Œéœ€è¦è°¨æ…å¤„ç†
            try:
                # é›ªçƒè‚¡ç¥¨è®¨è®ºé¡µé¢
                xueqiu_url = f"https://xueqiu.com/S/{ticker}"
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Referer': 'https://xueqiu.com/',
                }
                
                response = self.session.get(xueqiu_url, headers=headers, timeout=15)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # é›ªçƒçš„å®é™…HTMLç»“æ„å¯èƒ½ä¸åŒï¼Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                    # å°è¯•æŸ¥æ‰¾å¸–å­/è®¨è®ºé“¾æ¥
                    discussion_links = soup.find_all('a', href=re.compile(r'/status/|/article/|/stock/'))
                    
                    for link in discussion_links[:20]:  # é™åˆ¶æ•°é‡
                        try:
                            title = link.get_text(strip=True)
                            href = link.get('href', '')
                            
                            if title and len(title) > 5 and len(title) < 200:
                                discussions.append({
                                    'title': title,
                                    'content': title,
                                    'url': f"https://xueqiu.com{href}" if href and not href.startswith('http') else href,
                                    'source': 'é›ªçƒ'
                                })
                                
                                if any(keyword in title for keyword in ['æ¶¨', 'è·Œ', 'åˆ©å¥½', 'åˆ©ç©º', 'åˆ†æ', 'è§‚ç‚¹']):
                                    hot_topics.append(title[:50])
                        except:
                            continue
                    
                    if discussions:
                        logger.info(f"âœ… ä»é›ªçƒè·å–åˆ° {len(discussions)} æ¡è®¨è®º")
                        return {
                            'discussions': discussions,
                            'discussion_count': len(discussions),
                            'hot_topics': list(set(hot_topics))[:10],
                            'source': 'é›ªçƒ'
                        }
            
            except Exception as e:
                logger.debug(f"é›ªçƒæ•°æ®è·å–å¤±è´¥: {e}")
            
            return {
                'discussions': [],
                'discussion_count': 0,
                'hot_topics': [],
                'source': 'é›ªçƒï¼ˆå—é™ï¼‰'
            }
            
        except Exception as e:
            logger.debug(f"é›ªçƒæ•°æ®è·å–å¼‚å¸¸: {e}")
            return {
                'discussions': [],
                'discussion_count': 0,
                'hot_topics': [],
                'source': 'é›ªçƒ'
            }
    
    def _get_stock_info_from_akshare(self, ticker: str, days: int) -> Dict:
        """ä½¿ç”¨AKShareè·å–è‚¡ç¥¨ç›¸å…³ä¿¡æ¯ä½œä¸ºè¡¥å……"""
        try:
            import akshare as ak
            # è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ˆå¯ä»¥ä½œä¸ºè®¨è®ºçš„ä¸»é¢˜ï¼‰
            stock_info = ak.stock_individual_info_em(symbol=ticker)
            if stock_info is not None and not stock_info.empty:
                return {
                    'discussions': [{
                        'title': f'{ticker}è‚¡ç¥¨ä¿¡æ¯',
                        'content': 'é€šè¿‡AKShareè·å–çš„è‚¡ç¥¨ä¿¡æ¯',
                        'source': 'AKShare'
                    }],
                    'discussion_count': 1,
                    'hot_topics': [],
                    'source': 'AKShareè¡¥å……æ•°æ®'
                }
        except:
            pass
        return {
            'discussions': [],
            'discussion_count': 0,
            'hot_topics': [],
            'source': 'AKShare'
        }
    
    def _get_media_coverage_sentiment(self, ticker: str, days: int) -> Dict:
        """è·å–åª’ä½“æŠ¥é“æƒ…ç»ª"""
        try:
            # å¯ä»¥é›†æˆRSSæºæˆ–å…¬å¼€çš„è´¢ç»API
            coverage_items = self._get_media_coverage(ticker, days)
            
            if not coverage_items:
                return {'sentiment_score': 0, 'coverage_count': 0, 'confidence': 0}
            
            # åˆ†æåª’ä½“æŠ¥é“çš„æƒ…ç»ªå€¾å‘
            sentiment_scores = []
            for item in coverage_items:
                score = self._analyze_text_sentiment(item.get('title', '') + ' ' + item.get('summary', ''))
                sentiment_scores.append(score)
            
            avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0
            
            return {
                'sentiment_score': avg_sentiment,
                'coverage_count': len(coverage_items),
                'confidence': min(len(coverage_items) / 5, 1.0)
            }
            
        except Exception as e:
            return {'error': str(e), 'sentiment_score': 0, 'confidence': 0}
    
    def _search_finance_news(self, search_term: str, days: int) -> List[Dict]:
        """æœç´¢è´¢ç»æ–°é—» (ç¤ºä¾‹å®ç°)"""
        # è¿™é‡Œå¯ä»¥é›†æˆå¤šä¸ªæ–°é—»æºçš„APIæˆ–RSS
        # ä¾‹å¦‚ï¼šè´¢è”ç¤¾ã€æ–°æµªè´¢ç»ã€ä¸œæ–¹è´¢å¯Œç­‰
        
        # æ¨¡æ‹Ÿè¿”å›æ•°æ®ç»“æ„
        return [
            {
                'title': f'{search_term}ç›¸å…³è´¢ç»æ–°é—»æ ‡é¢˜',
                'content': 'æ–°é—»å†…å®¹æ‘˜è¦...',
                'source': 'è´¢è”ç¤¾',
                'publish_time': datetime.now().isoformat(),
                'url': 'https://example.com/news/1'
            }
        ]
    
    def _get_media_coverage(self, ticker: str, days: int) -> List[Dict]:
        """è·å–åª’ä½“æŠ¥é“ (ç¤ºä¾‹å®ç°)"""
        # å¯ä»¥é›†æˆGoogle News APIæˆ–å…¶ä»–æ–°é—»èšåˆæœåŠ¡
        return []
    
    def _analyze_text_sentiment(self, text: str) -> float:
        """ç®€å•çš„ä¸­æ–‡æ–‡æœ¬æƒ…ç»ªåˆ†æ"""
        if not text:
            return 0
        
        # ç®€å•çš„å…³é”®è¯æƒ…ç»ªåˆ†æ
        positive_words = ['ä¸Šæ¶¨', 'å¢é•¿', 'åˆ©å¥½', 'çœ‹å¥½', 'ä¹°å…¥', 'æ¨è', 'å¼ºåŠ¿', 'çªç ´', 'åˆ›æ–°é«˜']
        negative_words = ['ä¸‹è·Œ', 'ä¸‹é™', 'åˆ©ç©º', 'çœ‹ç©º', 'å–å‡º', 'é£é™©', 'è·Œç ´', 'åˆ›æ–°ä½', 'äºæŸ']
        
        positive_count = sum(1 for word in positive_words if word in text)
        negative_count = sum(1 for word in negative_words if word in text)
        
        if positive_count + negative_count == 0:
            return 0
        
        return (positive_count - negative_count) / (positive_count + negative_count)
    
    def _get_company_chinese_name(self, ticker: str) -> Optional[str]:
        """è·å–å…¬å¸ä¸­æ–‡åç§°"""
        # ç®€å•çš„æ˜ å°„è¡¨ï¼Œå®é™…å¯ä»¥ä»æ•°æ®åº“æˆ–APIè·å–
        name_mapping = {
            'AAPL': 'è‹¹æœ',
            'TSLA': 'ç‰¹æ–¯æ‹‰',
            'NVDA': 'è‹±ä¼Ÿè¾¾',
            'MSFT': 'å¾®è½¯',
            'GOOGL': 'è°·æ­Œ',
            'AMZN': 'äºšé©¬é€Š'
        }
        return name_mapping.get(ticker.upper())
    
    def _calculate_overall_sentiment(self, news_sentiment: Dict, forum_sentiment: Dict, media_sentiment: Dict) -> Dict:
        """è®¡ç®—ç»¼åˆæƒ…ç»ªåˆ†æ"""
        # æ ¹æ®å„æ•°æ®æºçš„ç½®ä¿¡åº¦åŠ æƒè®¡ç®—
        news_weight = news_sentiment.get('confidence', 0)
        forum_weight = forum_sentiment.get('confidence', 0)
        media_weight = media_sentiment.get('confidence', 0)
        
        total_weight = news_weight + forum_weight + media_weight
        
        if total_weight == 0:
            return {'sentiment_score': 0, 'confidence': 0, 'level': 'neutral'}
        
        weighted_sentiment = (
            news_sentiment.get('sentiment_score', 0) * news_weight +
            forum_sentiment.get('sentiment_score', 0) * forum_weight +
            media_sentiment.get('sentiment_score', 0) * media_weight
        ) / total_weight
        
        # ç¡®å®šæƒ…ç»ªç­‰çº§
        if weighted_sentiment > 0.3:
            level = 'very_positive'
        elif weighted_sentiment > 0.1:
            level = 'positive'
        elif weighted_sentiment > -0.1:
            level = 'neutral'
        elif weighted_sentiment > -0.3:
            level = 'negative'
        else:
            level = 'very_negative'
        
        return {
            'sentiment_score': weighted_sentiment,
            'confidence': total_weight / 3,  # å¹³å‡ç½®ä¿¡åº¦
            'level': level
        }
    
    def _generate_sentiment_summary(self, overall_sentiment: Dict) -> str:
        """ç”Ÿæˆæƒ…ç»ªåˆ†ææ‘˜è¦"""
        level = overall_sentiment.get('level', 'neutral')
        score = overall_sentiment.get('sentiment_score', 0)
        confidence = overall_sentiment.get('confidence', 0)
        
        level_descriptions = {
            'very_positive': 'éå¸¸ç§¯æ',
            'positive': 'ç§¯æ',
            'neutral': 'ä¸­æ€§',
            'negative': 'æ¶ˆæ',
            'very_negative': 'éå¸¸æ¶ˆæ'
        }
        
        description = level_descriptions.get(level, 'ä¸­æ€§')
        confidence_level = 'é«˜' if confidence > 0.7 else 'ä¸­' if confidence > 0.3 else 'ä½'
        
        return f"å¸‚åœºæƒ…ç»ª: {description} (è¯„åˆ†: {score:.2f}, ç½®ä¿¡åº¦: {confidence_level})"


def get_chinese_social_sentiment(ticker: str, curr_date: str) -> str:
    """
    è·å–ä¸­å›½ç¤¾äº¤åª’ä½“æƒ…ç»ªåˆ†æçš„ä¸»è¦æ¥å£å‡½æ•°
    """
    aggregator = ChineseFinanceDataAggregator()
    
    try:
        # è·å–æƒ…ç»ªåˆ†ææ•°æ®
        sentiment_data = aggregator.get_stock_sentiment_summary(ticker, days=7)
        
        # æ ¼å¼åŒ–è¾“å‡º
        if 'error' in sentiment_data:
            return f"""
ä¸­å›½å¸‚åœºæƒ…ç»ªåˆ†ææŠ¥å‘Š - {ticker}
åˆ†ææ—¥æœŸ: {curr_date}

âš ï¸ æ•°æ®è·å–é™åˆ¶è¯´æ˜:
{sentiment_data.get('fallback_message', 'æ•°æ®è·å–é‡åˆ°æŠ€æœ¯é™åˆ¶')}

å»ºè®®:
1. é‡ç‚¹å…³æ³¨è´¢ç»æ–°é—»å’ŒåŸºæœ¬é¢åˆ†æ
2. å‚è€ƒå®˜æ–¹è´¢æŠ¥å’Œä¸šç»©æŒ‡å¯¼
3. å…³æ³¨è¡Œä¸šæ”¿ç­–å’Œç›‘ç®¡åŠ¨æ€
4. è€ƒè™‘å›½é™…å¸‚åœºæƒ…ç»ªå¯¹ä¸­æ¦‚è‚¡çš„å½±å“

æ³¨: ç”±äºä¸­å›½ç¤¾äº¤åª’ä½“å¹³å°APIé™åˆ¶ï¼Œå½“å‰ä¸»è¦ä¾èµ–å…¬å¼€è´¢ç»æ•°æ®æºè¿›è¡Œåˆ†æã€‚
"""
        
        overall = sentiment_data.get('overall_sentiment', {})
        news = sentiment_data.get('news_sentiment', {})
        forum = sentiment_data.get('forum_sentiment', {})
        
        # æ„å»ºæŠ¥å‘Š
        report_lines = [
            f"ä¸­å›½å¸‚åœºæƒ…ç»ªåˆ†ææŠ¥å‘Š - {ticker}",
            f"åˆ†ææ—¥æœŸ: {curr_date}",
            f"åˆ†æå‘¨æœŸ: {sentiment_data.get('analysis_period', '7å¤©')}",
            "",
            f"ğŸ“Š ç»¼åˆæƒ…ç»ªè¯„ä¼°:",
            f"{sentiment_data.get('summary', 'æ•°æ®ä¸è¶³')}",
            "",
            f"ğŸ“° è´¢ç»æ–°é—»æƒ…ç»ª:",
            f"- æƒ…ç»ªè¯„åˆ†: {news.get('sentiment_score', 0):.2f}",
            f"- æ­£é¢æ–°é—»æ¯”ä¾‹: {news.get('positive_ratio', 0):.1%}",
            f"- è´Ÿé¢æ–°é—»æ¯”ä¾‹: {news.get('negative_ratio', 0):.1%}",
            f"- æ–°é—»æ•°é‡: {news.get('news_count', 0)}æ¡",
        ]
        
        # æ·»åŠ è‚¡å§è®¨è®ºæ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        if forum.get('discussion_count', 0) > 0:
            report_lines.extend([
                "",
                f"ğŸ’¬ è‚¡å§è®¨è®ºæƒ…ç»ª ({forum.get('platform', 'ä¸œæ–¹è´¢å¯Œè‚¡å§')}):",
                f"- æƒ…ç»ªè¯„åˆ†: {forum.get('sentiment_score', 0):.2f}",
                f"- è®¨è®ºæ•°é‡: {forum.get('discussion_count', 0)}æ¡",
                f"- æ•°æ®æ¥æº: {forum.get('platform', 'ä¸œæ–¹è´¢å¯Œè‚¡å§')}",
            ])
            
            hot_topics = forum.get('hot_topics', [])
            if hot_topics:
                report_lines.append(f"- çƒ­é—¨è¯é¢˜: {len(hot_topics)}ä¸ª")
                report_lines.append("  æœ€è¿‘çƒ­é—¨è®¨è®º:")
                for topic in hot_topics[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    report_lines.append(f"    â€¢ {topic[:60]}...")
        
        report_lines.extend([
            "",
            "ğŸ’¡ æŠ•èµ„å»ºè®®:",
            "åŸºäºå½“å‰å¯è·å–çš„ä¸­å›½å¸‚åœºæ•°æ®ï¼Œå»ºè®®æŠ•èµ„è€…:",
            "1. å¯†åˆ‡å…³æ³¨å®˜æ–¹è´¢ç»åª’ä½“æŠ¥é“",
            "2. é‡è§†åŸºæœ¬é¢åˆ†æå’Œè´¢åŠ¡æ•°æ®",
            "3. å‚è€ƒè‚¡å§æŠ•èµ„è€…è®¨è®ºï¼ˆéœ€ç»“åˆåŸºæœ¬é¢ï¼‰",
            "4. è€ƒè™‘æ”¿ç­–ç¯å¢ƒå¯¹è‚¡ä»·çš„å½±å“",
            "",
            f"ç”Ÿæˆæ—¶é—´: {sentiment_data.get('timestamp', datetime.now().isoformat())}",
        ])
        
        return "\n".join(report_lines)
        
    except Exception as e:
        return f"""
ä¸­å›½å¸‚åœºæƒ…ç»ªåˆ†æ - {ticker}
åˆ†ææ—¥æœŸ: {curr_date}

âŒ åˆ†æå¤±è´¥: {str(e)}

ğŸ’¡ æ›¿ä»£å»ºè®®:
1. æŸ¥çœ‹è´¢ç»æ–°é—»ç½‘ç«™çš„ç›¸å…³æŠ¥é“
2. å…³æ³¨é›ªçƒã€ä¸œæ–¹è´¢å¯Œç­‰æŠ•èµ„ç¤¾åŒºè®¨è®º
3. å‚è€ƒä¸“ä¸šæœºæ„çš„ç ”ç©¶æŠ¥å‘Š
4. é‡ç‚¹åˆ†æåŸºæœ¬é¢å’ŒæŠ€æœ¯é¢æ•°æ®

æ³¨: ä¸­å›½ç¤¾äº¤åª’ä½“æ•°æ®è·å–å­˜åœ¨æŠ€æœ¯é™åˆ¶ï¼Œå»ºè®®ä»¥åŸºæœ¬é¢åˆ†æä¸ºä¸»ã€‚
"""
