#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæ•°æ®ä¸‹è½½å™¨
æ”¯æŒParquetç¼“å­˜ã€å¢é‡æ›´æ–°ã€æ•°æ®éªŒè¯
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
import time

try:
    import pyarrow as pa
    import pyarrow.parquet as pq
    PARQUET_AVAILABLE = True
except ImportError:
    PARQUET_AVAILABLE = False

from tradingagents.utils.logging_init import get_logger
from tradingagents.dataflows.a_share_downloader import AShareDownloader

logger = get_logger('dataflows.data_downloader')


class DataDownloader:
    """
    å¢å¼ºç‰ˆæ•°æ®ä¸‹è½½å™¨
    æ”¯æŒæœ¬åœ°ç¼“å­˜ï¼ˆParquetï¼‰ã€å¢é‡æ›´æ–°ã€æ•°æ®éªŒè¯
    """
    
    def __init__(
        self,
        save_path: str = "data/stock_daily.parquet",
        cache_dir: str = "data/cache",
        provider: str = "tushare"
    ):
        """
        åˆå§‹åŒ–æ•°æ®ä¸‹è½½å™¨
        
        Args:
            save_path: ä¸»æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆParquetæ ¼å¼ï¼‰
            cache_dir: ç¼“å­˜ç›®å½•
            provider: æ•°æ®æä¾›å•†ï¼ˆtushare/akshareï¼‰
        """
        self.save_path = Path(save_path)
        self.save_path.parent.mkdir(parents=True, exist_ok=True)
        
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.provider = provider
        
        # åˆå§‹åŒ–æ•°æ®æä¾›å•†
        if provider == "tushare":
            try:
                from tradingagents.dataflows.tushare_adapter import get_tushare_adapter
                adapter = get_tushare_adapter()
                if adapter.provider and adapter.provider.connected:
                    self.pro = adapter.provider.pro_api
                    self.provider_available = True
                else:
                    self.pro = None
                    self.provider_available = False
            except Exception as e:
                logger.warning(f"âš ï¸ Tushareåˆå§‹åŒ–å¤±è´¥: {e}")
                self.pro = None
                self.provider_available = False
        else:
            self.pro = None
            self.provider_available = False
        
        # Aè‚¡åŸºç¡€æ•°æ®ä¸‹è½½å™¨ï¼ˆç”¨äºè·å–è‚¡ç¥¨åˆ—è¡¨ï¼‰
        self.basic_downloader = AShareDownloader()
        
        logger.info(f"âœ… DataDownloaderåˆå§‹åŒ–å®Œæˆ (provider={provider}, parquet={PARQUET_AVAILABLE})")
    
    def _load_existing_data(self) -> pd.DataFrame:
        """åŠ è½½ç°æœ‰æ•°æ®"""
        if not PARQUET_AVAILABLE:
            # é™çº§åˆ°CSV
            csv_path = str(self.save_path).replace('.parquet', '.csv')
            if os.path.exists(csv_path):
                return pd.read_csv(csv_path, parse_dates=['trade_date'])
            return pd.DataFrame()
        
        if not self.save_path.exists():
            return pd.DataFrame()
        
        try:
            return pq.read_table(self.save_path).to_pandas()
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å–Parquetå¤±è´¥ï¼Œå°è¯•CSV: {e}")
            csv_path = str(self.save_path).replace('.parquet', '.csv')
            if os.path.exists(csv_path):
                return pd.read_csv(csv_path, parse_dates=['trade_date'])
            return pd.DataFrame()
    
    def _save_data(self, df: pd.DataFrame, mode: str = "overwrite"):
        """ä¿å­˜æ•°æ®åˆ°Parquetæˆ–CSV"""
        if df.empty:
            return
        
        if PARQUET_AVAILABLE:
            try:
                if mode == "append" and self.save_path.exists():
                    # è¯»å–ç°æœ‰æ•°æ®å¹¶åˆå¹¶
                    existing = self._load_existing_data()
                    if not existing.empty:
                        # åˆå¹¶å¹¶å»é‡
                        combined = pd.concat([existing, df], ignore_index=True)
                        combined = combined.drop_duplicates(
                            subset=['ts_code', 'trade_date'],
                            keep='last'
                        ).sort_values(['ts_code', 'trade_date'])
                        df = combined
                
                table = pa.Table.from_pandas(df)
                pq.write_table(table, self.save_path)
                logger.info(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°Parquet: {len(df)} æ¡è®°å½•")
                return
            except Exception as e:
                logger.warning(f"âš ï¸ Parquetä¿å­˜å¤±è´¥ï¼Œé™çº§åˆ°CSV: {e}")
        
        # é™çº§åˆ°CSV
        csv_path = str(self.save_path).replace('.parquet', '.csv')
        if mode == "append" and os.path.exists(csv_path):
            existing = pd.read_csv(csv_path, parse_dates=['trade_date'])
            df = pd.concat([existing, df], ignore_index=True)
            df = df.drop_duplicates(
                subset=['ts_code', 'trade_date'],
                keep='last'
            ).sort_values(['ts_code', 'trade_date'])
        
        df.to_csv(csv_path, index=False)
        logger.info(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°CSV: {len(df)} æ¡è®°å½•")
    
    def update_daily(
        self,
        code_list: Optional[List[str]] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        force_refresh: bool = False
    ) -> pd.DataFrame:
        """
        æ›´æ–°æ¯æ—¥æ•°æ®
        
        Args:
            code_list: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ›´æ–°æ‰€æœ‰
            start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDï¼‰ï¼ŒNoneè¡¨ç¤ºä»æœ€æ–°æ•°æ®ç»§ç»­
            end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDï¼‰ï¼ŒNoneè¡¨ç¤ºä»Šå¤©
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
        
        Returns:
            æ›´æ–°çš„æ•°æ®DataFrame
        """
        if not self.provider_available:
            logger.error("âŒ æ•°æ®æä¾›å•†æœªå°±ç»ªï¼Œè¯·æ£€æŸ¥é…ç½®")
            return pd.DataFrame()
        
        # åŠ è½½ç°æœ‰æ•°æ®
        existing_data = self._load_existing_data() if not force_refresh else pd.DataFrame()
        
        # ç¡®å®šæ›´æ–°çš„è‚¡ç¥¨åˆ—è¡¨
        if code_list is None:
            # è·å–æ‰€æœ‰Aè‚¡ä»£ç 
            basic_info = self.basic_downloader.download_all_stocks(use_cache=True)
            if basic_info.empty:
                logger.error("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
                return pd.DataFrame()
            code_list = basic_info['ts_code'].tolist()
            logger.info(f"ğŸ“‹ è·å–åˆ° {len(code_list)} åªè‚¡ç¥¨")
        
        # ç¡®å®šæ—¥æœŸèŒƒå›´
        if end_date is None:
            end_date = datetime.now().strftime('%Y%m%d')
        
        if start_date is None:
            if not existing_data.empty:
                # ä»æœ€æ–°æ•°æ®ç»§ç»­
                latest_date = existing_data['trade_date'].max()
                if isinstance(latest_date, pd.Timestamp):
                    start_date = (latest_date + timedelta(days=1)).strftime('%Y%m%d')
                else:
                    start_date = (pd.to_datetime(latest_date) + timedelta(days=1)).strftime('%Y%m%d')
            else:
                # é»˜è®¤ä¸‹è½½æœ€è¿‘ä¸€å¹´
                start_date = (datetime.now() - timedelta(days=365)).strftime('%Y%m%d')
        
        logger.info(f"ğŸ“¥ å¼€å§‹æ›´æ–°æ•°æ®: {len(code_list)} åªè‚¡ç¥¨, {start_date} åˆ° {end_date}")
        
        # åˆ†æ‰¹ä¸‹è½½
        all_data = []
        batch_size = 500
        total_batches = (len(code_list) + batch_size - 1) // batch_size
        
        for i in range(0, len(code_list), batch_size):
            batch = code_list[i:i+batch_size]
            batch_num = i // batch_size + 1
            
            logger.info(f"â³ æ‰¹æ¬¡ {batch_num}/{total_batches}: å¤„ç† {len(batch)} åªè‚¡ç¥¨")
            
            for code in batch:
                try:
                    # æ£€æŸ¥ç¼“å­˜
                    cache_file = self.cache_dir / f"{code}_{start_date}_{end_date}.parquet"
                    if not force_refresh and cache_file.exists():
                        try:
                            cached_df = pq.read_table(cache_file).to_pandas()
                            if not cached_df.empty:
                                all_data.append(cached_df)
                                continue
                        except:
                            pass
                    
                    # ä»APIè·å–ï¼ˆå¸¦é‡è¯•ï¼‰
                    max_retries = 3
                    df = None
                    
                    for attempt in range(max_retries):
                        try:
                            df = self.pro.daily(
                                ts_code=code,
                                start_date=start_date,
                                end_date=end_date
                            )
                            
                            # æˆåŠŸè·å–æ•°æ®ï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                            break
                            
                        except Exception as api_error:
                            error_msg = str(api_error)
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯é¢‘ç‡é™åˆ¶é”™è¯¯
                            if "Too Many Requests" in error_msg or "Rate limited" in error_msg or "é¢‘ç‡é™åˆ¶" in error_msg:
                                if attempt < max_retries - 1:
                                    # æŒ‡æ•°é€€é¿ï¼š2ç§’ã€4ç§’ã€6ç§’
                                    wait_time = 2 * (attempt + 1)
                                    logger.warning(f"â³ {code} é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                                    time.sleep(wait_time)
                                    continue
                                else:
                                    logger.error(f"âŒ {code} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡")
                                    df = None
                                    break
                            else:
                                # å…¶ä»–é”™è¯¯ï¼Œç›´æ¥é€€å‡º
                                logger.error(f"âŒ {code} APIé”™è¯¯: {error_msg}")
                                df = None
                                break
                    
                    if df is not None and not df.empty:
                        # æ ‡å‡†åŒ–åˆ—å
                        df['ts_code'] = code
                        df['trade_date'] = pd.to_datetime(df['trade_date'])
                        
                        # ç¼“å­˜åˆ°æœ¬åœ°
                        if PARQUET_AVAILABLE:
                            try:
                                table = pa.Table.from_pandas(df)
                                pq.write_table(table, cache_file)
                            except:
                                pass
                        
                        all_data.append(df)
                    
                    # æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼ˆTushareè¦æ±‚é—´éš”0.2ç§’ä»¥ä¸Šï¼‰
                    time.sleep(0.3)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ {code} ä¸‹è½½å¤±è´¥: {e}")
                    continue
        
        if not all_data:
            logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            return pd.DataFrame()
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        result = pd.concat(all_data, ignore_index=True)
        result = result.drop_duplicates(subset=['ts_code', 'trade_date'], keep='last')
        
        # æ•°æ®éªŒè¯
        result = self._validate_data(result)
        
        # ä¿å­˜åˆ°ä¸»æ–‡ä»¶
        self._save_data(result, mode="append")
        
        logger.info(f"âœ… æ•°æ®æ›´æ–°å®Œæˆ: {len(result)} æ¡æ–°è®°å½•")
        return result
    
    def _validate_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        if df.empty:
            return df
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        required_cols = ['ts_code', 'trade_date', 'close']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            logger.warning(f"âš ï¸ æ•°æ®ç¼ºå°‘åˆ—: {missing_cols}")
            return pd.DataFrame()
        
        # åˆ é™¤å¼‚å¸¸æ•°æ®
        original_len = len(df)
        
        # åˆ é™¤ä»·æ ¼ä¸º0æˆ–è´Ÿæ•°çš„è®°å½•
        if 'close' in df.columns:
            df = df[df['close'] > 0]
        
        # åˆ é™¤äº¤æ˜“é‡ä¸ºè´Ÿæ•°çš„è®°å½•
        if 'vol' in df.columns:
            df = df[df['vol'] >= 0]
        
        # åˆ é™¤é‡å¤è®°å½•
        df = df.drop_duplicates(subset=['ts_code', 'trade_date'], keep='last')
        
        removed = original_len - len(df)
        if removed > 0:
            logger.info(f"ğŸ§¹ æ•°æ®éªŒè¯: åˆ é™¤äº† {removed} æ¡å¼‚å¸¸è®°å½•")
        
        return df
    
    def get_stock_data(
        self,
        symbol: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None
    ) -> pd.DataFrame:
        """
        è·å–å•åªè‚¡ç¥¨çš„æ•°æ®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDï¼‰
        
        Returns:
            è‚¡ç¥¨æ•°æ®DataFrame
        """
        # å…ˆä»ä¸»æ–‡ä»¶åŠ è½½
        existing = self._load_existing_data()
        
        if not existing.empty:
            # è¿‡æ»¤æŒ‡å®šè‚¡ç¥¨
            stock_data = existing[existing['ts_code'] == symbol].copy()
            
            # æŒ‰æ—¥æœŸè¿‡æ»¤
            if start_date:
                start = pd.to_datetime(start_date)
                stock_data = stock_data[stock_data['trade_date'] >= start]
            
            if end_date:
                end = pd.to_datetime(end_date)
                stock_data = stock_data[stock_data['trade_date'] <= end]
            
            if not stock_data.empty:
                logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½ {symbol}: {len(stock_data)} æ¡è®°å½•")
                return stock_data.sort_values('trade_date')
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œä»APIè·å–
        logger.info(f"ğŸ“¥ ä»APIè·å– {symbol} æ•°æ®...")
        return self.update_daily([symbol], start_date, end_date, force_refresh=False)
    
    def check_data_completeness(
        self,
        code_list: Optional[List[str]] = None,
        target_date: Optional[str] = None
    ) -> pd.DataFrame:
        """
        æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        
        Args:
            code_list: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            target_date: ç›®æ ‡æ—¥æœŸï¼ˆYYYYMMDDï¼‰ï¼ŒNoneè¡¨ç¤ºæœ€æ–°æ—¥æœŸ
        
        Returns:
            æ•°æ®å®Œæ•´æ€§æŠ¥å‘Š
        """
        if code_list is None:
            basic_info = self.basic_downloader.download_all_stocks(use_cache=True)
            code_list = basic_info['ts_code'].tolist()
        
        existing = self._load_existing_data()
        
        if existing.empty:
            return pd.DataFrame({
                'ts_code': code_list,
                'has_data': False,
                'latest_date': None,
                'record_count': 0
            })
        
        if target_date:
            target = pd.to_datetime(target_date)
        else:
            target = existing['trade_date'].max()
        
        report = []
        for code in code_list:
            stock_data = existing[existing['ts_code'] == code]
            has_data = not stock_data.empty
            latest_date = stock_data['trade_date'].max() if has_data else None
            record_count = len(stock_data)
            
            report.append({
                'ts_code': code,
                'has_data': has_data,
                'latest_date': latest_date,
                'record_count': record_count,
                'is_up_to_date': latest_date >= target if latest_date else False
            })
        
        return pd.DataFrame(report)


def get_data_downloader(
    save_path: str = "data/stock_daily.parquet",
    provider: str = "tushare"
) -> DataDownloader:
    """è·å–æ•°æ®ä¸‹è½½å™¨å®ä¾‹"""
    return DataDownloader(save_path=save_path, provider=provider)

