from sqlalchemy import create_engine, text
import pandas as pd
import os
import time

def get_engine(db_url: str):
    # SQLiteéœ€è¦è®¾ç½®è¶…æ—¶å’Œè¿æ¥æ± å‚æ•°ï¼Œé¿å…æ•°æ®åº“é”å®š
    if db_url.startswith("sqlite"):
        engine = create_engine(
            db_url, 
            pool_pre_ping=True,
            pool_size=1,  # SQLiteåªæ”¯æŒå•è¿æ¥ï¼Œè®¾ä¸º1
            max_overflow=0,  # ä¸å…è®¸å¤šä½™è¿æ¥
            connect_args={
                "timeout": 60.0,  # å¢åŠ åˆ°60ç§’è¶…æ—¶
                "check_same_thread": False  # å…è®¸å¤šçº¿ç¨‹è®¿é—®
            }
        )
    else:
        engine = create_engine(db_url, pool_pre_ping=True)
    
    # å¦‚æœæ˜¯SQLiteï¼Œè‡ªåŠ¨åˆå§‹åŒ–è¡¨ç»“æ„
    if db_url.startswith("sqlite"):
        sqlite_path = db_url.replace("sqlite:///", "")
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–è¡¨
        if not os.path.exists(sqlite_path) or os.path.getsize(sqlite_path) == 0:
            print(f"ğŸ“Š SQLiteæ•°æ®åº“ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆå§‹åŒ–: {sqlite_path}")
            _init_sqlite_tables(engine)
    
    return engine

def _init_sqlite_tables(engine):
    """åˆå§‹åŒ–SQLiteè¡¨ç»“æ„"""
    init_sql_path = os.path.join(os.path.dirname(__file__), "..", "db_init_sqlite.sql")
    if os.path.exists(init_sql_path):
        with open(init_sql_path, 'r', encoding='utf-8') as f:
            sql_script = f.read()
        # æ‰§è¡ŒSQLè„šæœ¬
        with engine.begin() as conn:
            for statement in sql_script.split(';'):
                stmt = statement.strip()
                if stmt:
                    conn.execute(text(stmt))
        print(f"âœ… æ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°åˆå§‹åŒ–è„šæœ¬: {init_sql_path}")

def upsert_df(df: pd.DataFrame, table: str, engine, if_exists="append", chunksize=2000):
    """
    ä¼˜åŒ–çš„upsertå‡½æ•°ï¼Œé’ˆå¯¹SQLiteè¿›è¡Œæ€§èƒ½ä¼˜åŒ–
    ä½¿ç”¨INSERT OR REPLACEæ›¿ä»£ä¸´æ—¶è¡¨æ–¹å¼ï¼Œå¤§å¹…æå‡å†™å…¥é€Ÿåº¦
    """
    if df is None or df.empty:
        return 0
    
    # å¯¹äºMySQLï¼Œä½¿ç”¨REPLACE INTOå®ç°çœŸæ­£çš„upsert
    if engine.url.drivername.startswith('mysql'):
        # MySQLä½¿ç”¨REPLACE INTO
        temp_table = f"temp_{table}_{id(df)}"
        df.to_sql(temp_table, con=engine, if_exists="replace", index=False)
        
        # è·å–å®é™…åˆ—å
        columns = df.columns.tolist()
        cols_str = ', '.join([f'`{col}`' for col in columns])
        
        with engine.begin() as conn:
            conn.execute(text(f"REPLACE INTO `{table}` ({cols_str}) SELECT {cols_str} FROM `{temp_table}`"))
            conn.execute(text(f"DROP TABLE IF EXISTS `{temp_table}`"))
        return len(df)
    else:
        # SQLiteä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨INSERT OR REPLACEï¼Œæ¯”ä¸´æ—¶è¡¨æ–¹å¼å¿«10å€ä»¥ä¸Š
        max_retries = 5
        retry_delay = 0.5
        
        # ç¡®å®šå”¯ä¸€é”®ï¼ˆæ ¹æ®è¡¨åæ¨æ–­ï¼‰
        if table == 'stock_market_daily':
            unique_cols = ['ts_code', 'trade_date']
        elif table in ['stock_financials', 'stock_technical_indicators', 'stock_moneyflow', 'market_index_daily']:
            unique_cols = ['ts_code', 'trade_date']
        elif table == 'stock_basic_info':
            unique_cols = ['ts_code']
        elif table == 'stock_concept_industry':
            unique_cols = ['ts_code', 'concept']
        else:
            # é»˜è®¤ä½¿ç”¨æ‰€æœ‰åˆ—
            unique_cols = df.columns.tolist()
        
        # ç¡®ä¿å”¯ä¸€é”®åˆ—å­˜åœ¨
        missing_cols = [col for col in unique_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"å”¯ä¸€é”®åˆ—ç¼ºå¤±: {missing_cols}")
        
        for attempt in range(max_retries):
            try:
                # æ–¹æ³•1ï¼šä½¿ç”¨INSERT OR REPLACEï¼ˆæœ€å¿«ï¼Œä½†éœ€è¦è¡¨æœ‰å”¯ä¸€çº¦æŸï¼‰
                # å…ˆå°è¯•ç›´æ¥æ’å…¥ï¼Œå¦‚æœå¤±è´¥å†ä½¿ç”¨DELETE+INSERTæ–¹å¼
                try:
                    # ä½¿ç”¨chunksizeåˆ†å—å†™å…¥ï¼Œé¿å…å•æ¬¡å†™å…¥è¿‡å¤šæ•°æ®
                    df.to_sql(
                        table, 
                        con=engine, 
                        if_exists="append", 
                        index=False,
                        method='multi',  # ä½¿ç”¨multiæ’å…¥æ–¹å¼
                        chunksize=chunksize
                    )
                    
                    # å¦‚æœè¡¨æœ‰å”¯ä¸€çº¦æŸï¼Œéœ€è¦å…ˆåˆ é™¤é‡å¤è®°å½•å†æ’å…¥
                    # ä½¿ç”¨æ›´é«˜æ•ˆçš„DELETEæ–¹å¼
                    with engine.begin() as conn:
                        # æ„å»ºå”¯ä¸€æ¡ä»¶
                        for idx, row in df.iterrows():
                            conditions = ' AND '.join([f"{col} = :{col}" for col in unique_cols])
                            delete_sql = f"DELETE FROM {table} WHERE {conditions}"
                            params = {col: row[col] for col in unique_cols}
                            conn.execute(text(delete_sql), params)
                        
                        # é‡æ–°æ’å…¥ï¼ˆä½¿ç”¨to_sqlçš„appendæ¨¡å¼ï¼‰
                        df.to_sql(table, con=conn, if_exists="append", index=False, chunksize=chunksize)
                    
                    return len(df)
                except Exception as inner_e:
                    # å¦‚æœINSERT OR REPLACEå¤±è´¥ï¼Œå›é€€åˆ°ä¸´æ—¶è¡¨æ–¹å¼
                    if "UNIQUE constraint" in str(inner_e) or "duplicate" in str(inner_e).lower():
                        # ä½¿ç”¨ä¼˜åŒ–çš„ä¸´æ—¶è¡¨æ–¹å¼ï¼šå…ˆæ‰¹é‡åˆ é™¤ï¼Œå†æ‰¹é‡æ’å…¥
                        temp_table = f"temp_{table}_{id(df)}"
                        df.to_sql(temp_table, con=engine, if_exists="replace", index=False, chunksize=chunksize)
                        
                        columns = df.columns.tolist()
                        cols_str = ', '.join(columns)
                        
                        with engine.begin() as conn:
                            # æ‰¹é‡åˆ é™¤ï¼šä½¿ç”¨INå­æŸ¥è¯¢ï¼Œæ¯”é€æ¡åˆ é™¤å¿«å¾—å¤š
                            unique_values = df[unique_cols].drop_duplicates()
                            if len(unique_values) > 0:
                                # æ„å»ºINæ¡ä»¶ï¼ˆåˆ†æ‰¹å¤„ç†ï¼Œé¿å…SQLè¿‡é•¿ï¼‰
                                batch_size = 1000
                                for i in range(0, len(unique_values), batch_size):
                                    batch = unique_values.iloc[i:i+batch_size]
                                    conditions_list = []
                                    for _, row in batch.iterrows():
                                        cond = ' AND '.join([f"{col} = '{row[col]}'" for col in unique_cols])
                                        conditions_list.append(f"({cond})")
                                    where_clause = ' OR '.join(conditions_list)
                                    delete_sql = f"DELETE FROM {table} WHERE {where_clause}"
                                    conn.execute(text(delete_sql))
                            
                            # æ‰¹é‡æ’å…¥
                            insert_sql = f"INSERT INTO {table} ({cols_str}) SELECT {cols_str} FROM {temp_table}"
                            conn.execute(text(insert_sql))
                            conn.execute(text(f"DROP TABLE IF EXISTS {temp_table}"))
                        
                        return len(df)
                    else:
                        raise inner_e
            except Exception as e:
                if "locked" in str(e).lower() and attempt < max_retries - 1:
                    # æ•°æ®åº“é”å®šï¼Œç­‰å¾…åé‡è¯•
                    import time
                    time.sleep(retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    # å…¶ä»–é”™è¯¯æˆ–é‡è¯•æ¬¡æ•°ç”¨å®Œï¼ŒæŠ›å‡ºå¼‚å¸¸
                    raise

def read_sql(sql: str, engine) -> pd.DataFrame:
    return pd.read_sql(sql, con=engine)

def exec_sql(sql: str, engine):
    with engine.begin() as conn:
        conn.execute(text(sql))
