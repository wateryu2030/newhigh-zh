"""
BaoStockæ•°æ®ä¸‹è½½æ¨¡å—
åªä½¿ç”¨BaoStockä½œä¸ºå”¯ä¸€æ•°æ®æºï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
"""
import os
import sys
import time
import signal
from datetime import datetime, timedelta
import pandas as pd
from pathlib import Path

# Add data_engine directory to path for imports
data_engine_dir = Path(__file__).parent
sys.path.insert(0, str(data_engine_dir))

from utils.logger import setup_logger
from utils.retry import retry
from utils.db_utils import get_engine, upsert_df
from utils.fast_db_writer import fast_upsert_df, AsyncDBWriter

from config import DB_URL, START_DATE, END_DATE, SLEEP_SEC_WEB, DB_WRITE_BATCH_SIZE, CACHE_BATCH_SIZE

import baostock as bs

logger = setup_logger(log_file=os.path.join(os.path.dirname(__file__), "data_cache/update.log"))
engine = get_engine(DB_URL)


# ------------------ åŸºç¡€ä¿¡æ¯ ------------------
@retry(tries=5, delay=1.0)
def fetch_stock_basic_info():
    """è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ï¼Œåªä¿ç•™type=1çš„è‚¡ç¥¨ï¼Œæ’é™¤æŒ‡æ•°"""
    logger.info("å¼€å§‹è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯...")
    lg = bs.login()
    if lg.error_code != '0':
        raise RuntimeError(f"baostock ç™»å½•å¤±è´¥: {lg.error_msg}")
    
    rs = bs.query_stock_basic()
    rows = []
    while rs.error_code == '0' and rs.next():
        rows.append(rs.get_row_data())
    
    if not rows:
        bs.logout()
        raise RuntimeError("æœªè·å–åˆ°è‚¡ç¥¨åŸºç¡€ä¿¡æ¯")
    
    df = pd.DataFrame(rows, columns=rs.fields)
    bs.logout()
    
    # è¿‡æ»¤ï¼šåªä¿ç•™è‚¡ç¥¨ï¼ˆtype=1ï¼‰ï¼Œæ’é™¤æŒ‡æ•°ï¼ˆtype=2ï¼‰
    if 'type' in df.columns:
        df = df[df['type'] == '1']
        logger.info(f"è¿‡æ»¤åå‰©ä½™è‚¡ç¥¨: {len(df)} åªï¼ˆæ’é™¤äº†æŒ‡æ•°ï¼‰")
    
    # baostockçš„codeè½¬ts_codeé£æ ¼
    def to_ts(code):
        p = str(code).split('.')
        return (p[1] + ('.SH' if p[0]=='sh' else '.SZ')) if len(p)==2 else code
    
    df['ts_code'] = df['code'].map(to_ts)
    
    # æ ¹æ®å®é™…è¡¨ç»“æ„è°ƒæ•´å­—æ®µï¼ˆé€‚é…ç°æœ‰è¡¨ç»“æ„ï¼‰
    # å®é™…è¡¨æœ‰: code, code_name, ipoDate, outDate, type, status, ts_code
    # ä¿ç•™åŸå§‹å­—æ®µï¼Œåªæ·»åŠ ts_code
    df = df.drop_duplicates(subset=['ts_code'])
    
    # ä½¿ç”¨upserté€»è¾‘ï¼Œæ”¯æŒå¢é‡æ›´æ–°ï¼ˆç›´æ¥ä½¿ç”¨åŸå§‹å­—æ®µï¼‰
    upsert_df(df[['ts_code', 'code', 'code_name', 'ipoDate', 'outDate', 'type', 'status']], 
              "stock_basic_info", engine, if_exists="append")
    logger.info(f"stock_basic_info å†™å…¥ {len(df)} æ¡")
    return df[['ts_code']].dropna()


# ------------------ æ—¥è¡Œæƒ…ï¼ˆKçº¿+è´¢åŠ¡æŒ‡æ ‡ï¼‰ ------------------
@retry(tries=5, delay=1.0)
def fetch_market_daily(ts_codes: pd.Series):
    """è·å–æ—¥Kçº¿æ•°æ®ï¼ŒåŒ…å«PE/PB/PSç­‰è´¢åŠ¡æŒ‡æ ‡"""
    logger.info(f"å¼€å§‹ä¸‹è½½æ—¥Kè¡Œæƒ…æ•°æ®ï¼Œå…±{len(ts_codes)}åªè‚¡ç¥¨")
    
    # BaoStockæ ‡å‡†å­—æ®µåˆ—è¡¨
    fields = "date,code,open,high,low,close,preclose,volume,amount,turn,pctChg,peTTM,pbMRQ,psTTM"
    
    total = 0
    success_count = 0
    failed_count = 0
    failed_codes = []
    processed_count = 0  # å·²å¤„ç†çš„è‚¡ç¥¨æ€»æ•°ï¼ˆåŒ…æ‹¬æˆåŠŸå’Œå¤±è´¥ï¼‰
    
    # ä½¿ç”¨å¼‚æ­¥å†™å…¥å™¨ï¼ˆæ–‡ä»¶ç¼“å­˜ + å¼‚æ­¥æ‰¹é‡å†™å…¥ï¼‰
    cache_dir = os.path.join(os.path.dirname(__file__), "data_cache", "db_cache")
    market_daily_writer = AsyncDBWriter(
        engine, 
        "stock_market_daily", 
        cache_dir=cache_dir,
        cache_batch_size=CACHE_BATCH_SIZE,
        flush_interval=60  # 60ç§’è‡ªåŠ¨åˆ·æ–°
    )
    financials_writer = AsyncDBWriter(
        engine,
        "stock_financials",
        cache_dir=cache_dir,
        cache_batch_size=CACHE_BATCH_SIZE,
        flush_interval=60
    )
    
    # ä¸å†éœ€è¦å†…å­˜æ‰¹æ¬¡ï¼Œç›´æ¥ä½¿ç”¨å¼‚æ­¥å†™å…¥å™¨
    
    # ç™»å½•çŠ¶æ€æ£€æŸ¥å’Œé‡æ–°ç™»å½•
    login_retry_count = 0
    max_login_retries = 3
    
    def ensure_login():
        """ç¡®ä¿BaoStockå·²ç™»å½•ï¼Œå¦‚æœæœªç™»å½•åˆ™é‡æ–°ç™»å½•"""
        nonlocal login_retry_count
        bs.logout()  # å…ˆç™»å‡º
        time.sleep(0.5)
        lg = bs.login()
        if lg.error_code != '0':
            login_retry_count += 1
            if login_retry_count >= max_login_retries:
                raise RuntimeError(f"BaoStockç™»å½•å¤±è´¥ï¼ˆé‡è¯•{max_login_retries}æ¬¡ï¼‰: {lg.error_msg}")
            logger.warning(f"é‡æ–°ç™»å½•å¤±è´¥ï¼Œ{login_retry_count}/{max_login_retries}æ¬¡é‡è¯•")
            time.sleep(1)
            return ensure_login()  # é€’å½’é‡è¯•
        else:
            login_retry_count = 0
            logger.info("âœ… é‡æ–°ç™»å½•æˆåŠŸ")
    
    # åˆå§‹ç™»å½•
    lg = bs.login()
    if lg.error_code != '0':
        raise RuntimeError(f"baostock ç™»å½•å¤±è´¥: {lg.error_msg}")
    
    # ç¡®ä¿ts_codesæ˜¯Seriesæ ¼å¼
    if isinstance(ts_codes, pd.DataFrame):
        ts_codes = ts_codes['ts_code'] if 'ts_code' in ts_codes.columns else ts_codes.iloc[:, 0]
    
    for i, ts_code in enumerate(ts_codes, 0):
        # è½¬baostockä»£ç é£æ ¼
        if ts_code.endswith(".SH"):
            code = "sh." + ts_code.split('.')[0]
        else:
            code = "sz." + ts_code.split('.')[0]
        
        processed_count += 1  # æ¯å¤„ç†ä¸€åªè‚¡ç¥¨å°±é€’å¢
        
        try:
            # è·å–Kçº¿æ•°æ®å’Œè´¢åŠ¡æŒ‡æ ‡ï¼ˆæ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼‰
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError(f"æŸ¥è¯¢è¶…æ—¶: {ts_code}")
            
            # è®¾ç½®5ç§’è¶…æ—¶ï¼ˆå•åªè‚¡ç¥¨æŸ¥è¯¢ï¼‰
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(5)  # 5ç§’è¶…æ—¶
            
            try:
                rs = bs.query_history_k_data_plus(
                    code,
                    fields,
                    start_date=START_DATE,
                    end_date=END_DATE,
                    frequency="d",
                    adjustflag="3"
                )
                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
            except TimeoutError:
                logger.warning(f"  âš ï¸ {ts_code} ({code}) æŸ¥è¯¢è¶…æ—¶")
                failed_count += 1
                failed_codes.append(ts_code)
                time.sleep(SLEEP_SEC_WEB)
                continue
            except Exception as e:
                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                raise e
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if rs.error_code != '0':
                # å¦‚æœæ˜¯"ç”¨æˆ·æœªç™»å½•"é”™è¯¯ï¼Œå°è¯•é‡æ–°ç™»å½•
                if 'æœªç™»å½•' in rs.error_msg or 'ç”¨æˆ·æœªç™»å½•' in rs.error_msg:
                    logger.warning(f"  âš ï¸ {ts_code} ({code}) ç™»å½•å¤±æ•ˆï¼Œå°è¯•é‡æ–°ç™»å½•...")
                    try:
                        ensure_login()
                        # é‡æ–°å°è¯•æŸ¥è¯¢
                        rs = bs.query_history_k_data_plus(
                            code,
                            fields,
                            start_date=START_DATE,
                            end_date=END_DATE,
                            frequency="d",
                            adjustflag="3"
                        )
                        if rs.error_code == '0':
                            # é‡æ–°ç™»å½•æˆåŠŸï¼Œç»§ç»­å¤„ç†
                            pass
                        else:
                            logger.warning(f"  âš ï¸ {ts_code} ({code}) é‡æ–°ç™»å½•åæŸ¥è¯¢ä»å¤±è´¥: {rs.error_msg}")
                            failed_count += 1
                            failed_codes.append(ts_code)
                            time.sleep(SLEEP_SEC_WEB * 2)
                            continue
                    except Exception as e:
                        logger.error(f"  âŒ {ts_code} ({code}) é‡æ–°ç™»å½•å¤±è´¥: {e}")
                        failed_count += 1
                        failed_codes.append(ts_code)
                        time.sleep(SLEEP_SEC_WEB * 2)
                        continue
                # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯ï¼Œå¢åŠ ç­‰å¾…æ—¶é—´åé‡è¯•
                elif 'ç½‘ç»œæ¥æ”¶é”™è¯¯' in rs.error_msg or 'æ¥æ”¶æ•°æ®å¼‚å¸¸' in rs.error_msg:
                    logger.warning(f"  âš ï¸ {ts_code} ({code}) ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾…åé‡è¯•...")
                    time.sleep(SLEEP_SEC_WEB * 5)  # ç½‘ç»œé”™è¯¯æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
                    # å°è¯•é‡æ–°ç™»å½•
                    try:
                        ensure_login()
                        # é‡æ–°å°è¯•æŸ¥è¯¢
                        rs = bs.query_history_k_data_plus(
                            code,
                            fields,
                            start_date=START_DATE,
                            end_date=END_DATE,
                            frequency="d",
                            adjustflag="3"
                        )
                        if rs.error_code == '0':
                            # é‡è¯•æˆåŠŸï¼Œç»§ç»­å¤„ç†
                            pass
                        else:
                            logger.warning(f"  âš ï¸ {ts_code} ({code}) é‡è¯•åä»å¤±è´¥: {rs.error_msg}")
                            failed_count += 1
                            failed_codes.append(ts_code)
                            time.sleep(SLEEP_SEC_WEB * 2)
                            continue
                    except Exception as e:
                        logger.error(f"  âŒ {ts_code} ({code}) é‡è¯•å¤±è´¥: {e}")
                        failed_count += 1
                        failed_codes.append(ts_code)
                        time.sleep(SLEEP_SEC_WEB * 2)
                        continue
                else:
                    logger.warning(f"  âš ï¸ {ts_code} ({code}) æŸ¥è¯¢å¤±è´¥: {rs.error_msg}")
                    failed_count += 1
                    failed_codes.append(ts_code)
                    # ä¼˜åŒ–ï¼šå¤±è´¥æ—¶çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¿ç»­å¤±è´¥
                    time.sleep(SLEEP_SEC_WEB * 2)
                    continue
            
            rows = []
            while rs.error_code == '0' and rs.next():
                rows.append(rs.get_row_data())
            
            if rows:
                df = pd.DataFrame(rows, columns=rs.fields)
                # æ ‡å‡†åˆ—åæ˜ å°„
                df.rename(columns={
                    "date": "trade_date",
                    "pctChg": "pct_chg",
                    "turn": "turnover_rate"
                }, inplace=True)
                df["ts_code"] = ts_code
                
                # åˆ é™¤codeåˆ—
                if 'code' in df.columns:
                    df = df.drop(columns=['code'])
                
                # ç¡®ä¿æ‰€æœ‰æ•°å€¼åˆ—ä¸ºæ•°å€¼ç±»å‹
                numeric_cols = ['open', 'high', 'low', 'close', 'preclose', 'pct_chg', 'volume', 
                              'amount', 'turnover_rate', 'peTTM', 'pbMRQ', 'psTTM']
                for col in numeric_cols:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                
                # è®¡ç®—æŒ¯å¹…
                df['amplitude'] = ((df['high'] - df['low']) / df['preclose'] * 100).round(2)
                
                # ç¡®ä¿åˆ—é¡ºåºï¼ˆåŒ¹é…æ•°æ®åº“ï¼‰
                expected_fields = ['ts_code', 'trade_date', 'open', 'high', 'low', 'close', 'preclose',
                                 'pct_chg', 'volume', 'amount', 'turnover_rate', 'amplitude', 
                                 'peTTM', 'pbMRQ', 'psTTM']
                for field in expected_fields:
                    if field not in df.columns:
                        df[field] = None
                df = df[expected_fields]
                
                # ç›´æ¥ä½¿ç”¨å¼‚æ­¥å†™å…¥å™¨ï¼ˆéé˜»å¡ï¼‰
                market_daily_writer.append(df)
                
                # åŒæ—¶å‡†å¤‡è´¢åŠ¡æ•°æ®
                if 'peTTM' in df.columns or 'pbMRQ' in df.columns:
                    df_fin = df[['ts_code', 'trade_date']].copy()
                    if 'peTTM' in df.columns:
                        df_fin['pe'] = df['peTTM']
                    if 'pbMRQ' in df.columns:
                        df_fin['pb'] = df['pbMRQ']
                    if 'psTTM' in df.columns:
                        df_fin['ps'] = df['psTTM']
                    
                    # è¡¥å……å…¶ä»–å­—æ®µ
                    for col in ['pcf', 'roe', 'roa', 'eps', 'bps', 'total_mv', 'circ_mv', 
                              'revenue_yoy', 'net_profit_yoy', 'gross_profit_margin']:
                        df_fin[col] = None
                    
                    df_fin = df_fin.dropna(subset=['pe', 'pb'], how='all')
                    if not df_fin.empty:
                        financials_writer.append(df_fin)
                
                total += len(df)
                
                # æ·»åŠ æˆåŠŸæ—¥å¿—ï¼ˆæ¯10åªè‚¡ç¥¨è®°å½•ä¸€æ¬¡ï¼Œé¿å…æ—¥å¿—è¿‡å¤šï¼‰
                if success_count % 10 == 0:
                    logger.info(f"  âœ… {ts_code} ({code}) ä¸‹è½½æˆåŠŸï¼Œå…± {len(df)} å¤©æ•°æ®")
                success_count += 1
            else:
                logger.warning(f"  âš ï¸ {ts_code} ({code}) æ— æ•°æ®è¿”å›")
                failed_count += 1
                failed_codes.append(ts_code)
        except Exception as e:
            logger.error(f"  âŒ {ts_code} ({code}) ä¸‹è½½å‡ºé”™: {e}")
            failed_count += 1
            failed_codes.append(ts_code)
        
        # æ¯50åªè‚¡ç¥¨æ˜¾ç¤ºè¿›åº¦ï¼Œå¹¶è§¦å‘åˆ·æ–°ï¼ˆç¡®ä¿æ•°æ®åŠæ—¶å†™å…¥ï¼‰
        if (i+1) % 50 == 0:
            logger.info(f"æ—¥Kè¿›åº¦ {i+1}/{len(ts_codes)} | æˆåŠŸ: {success_count} | å¤±è´¥: {failed_count}")
            # è§¦å‘åˆ·æ–°ï¼Œç¡®ä¿æ•°æ®åŠæ—¶å†™å…¥
            market_daily_writer.flush()
            financials_writer.flush()
        
        # ä¼˜åŒ–ï¼šå‡å°‘ä¼‘æ¯æ—¶é—´ï¼Œåªåœ¨æ¯200åªè‚¡ç¥¨æ—¶çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¢«é™æµ
        if (i+1) % 200 == 0:
            time.sleep(SLEEP_SEC_WEB * 3)  # æ¯200åªä¼‘æ¯0.15ç§’
        else:
            time.sleep(SLEEP_SEC_WEB)  # æ­£å¸¸é—´éš”0.05ç§’
    
    # æœ€ååˆ·æ–°æ‰€æœ‰ç¼“å­˜æ•°æ®
    logger.info("åˆ·æ–°æ‰€æœ‰ç¼“å­˜æ•°æ®...")
    market_daily_writer.flush()
    financials_writer.flush()
    
    # ç­‰å¾…æ‰€æœ‰å¼‚æ­¥å†™å…¥å®Œæˆ
    logger.info("ç­‰å¾…å¼‚æ­¥å†™å…¥å®Œæˆ...")
    
    # ç­‰å¾…é˜Ÿåˆ—æ¸…ç©º
    timeout = 300  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
    start_time = time.time()
    while not market_daily_writer.write_queue.empty() or not financials_writer.write_queue.empty():
        if time.time() - start_time > timeout:
            logger.warning("å¼‚æ­¥å†™å…¥è¶…æ—¶ï¼Œéƒ¨åˆ†æ•°æ®å¯èƒ½ä»åœ¨å†™å…¥é˜Ÿåˆ—ä¸­")
            break
        time.sleep(1)
    
    # åœæ­¢å†™å…¥å™¨
    market_daily_writer.stop()
    financials_writer.stop()
    logger.info("å¼‚æ­¥å†™å…¥å™¨å·²åœæ­¢")
    
    bs.logout()
    logger.info(f"stock_market_daily å…±å†™å…¥ {total} è¡Œ")
    logger.info(f"âœ… ä¸‹è½½å®Œæˆ: æˆåŠŸ {success_count} åªï¼Œå¤±è´¥ {failed_count} åª")
    if failed_codes:
        logger.warning(f"å¤±è´¥çš„è‚¡ç¥¨ä»£ç ï¼ˆå‰20ä¸ªï¼‰: {failed_codes[:20]}")
    return total


def main():
    """ä¸»å‡½æ•°ï¼šä¸‹è½½æ‰€æœ‰æ•°æ®"""
    logger.info("ğŸš€ å¼€å§‹æ›´æ–° Aè‚¡æ™ºèƒ½é€‰è‚¡åŸºç¡€æ•°æ®åº“")
    logger.info(f"æŠ“å–çª—å£: {START_DATE} ~ {END_DATE}")
    
    # 1. è·å–åŸºç¡€ä¿¡æ¯ï¼ˆé™æ€æ•°æ®ï¼Œå…¨é‡æ›´æ–°ï¼‰
    codes_df = fetch_stock_basic_info()
    
    # 2. ä¸‹è½½æ—¥Kè¡Œæƒ…ï¼ˆæ™ºèƒ½å¢é‡æ›´æ–°ï¼šä¼˜å…ˆä¸‹è½½ç¼ºå¤±çš„æ•°æ®ï¼‰
    # æ£€æŸ¥æ•°æ®åº“ä¸­å·²æœ‰çš„è‚¡ç¥¨ï¼Œåªä¸‹è½½ç¼ºå¤±çš„æ•°æ®
    try:
        from sqlalchemy import text
        # ä½¿ç”¨MySQLæ£€æŸ¥å·²æœ‰æ•°æ®
        with engine.connect() as conn:
            # è·å–å·²æœ‰æœ€æ–°æ—¥æœŸçš„è‚¡ç¥¨åˆ—è¡¨
            result = conn.execute(text("""
                SELECT DISTINCT ts_code 
                FROM stock_market_daily 
                WHERE trade_date = (SELECT MAX(trade_date) FROM stock_market_daily)
            """))
            existing_codes = set([row[0] for row in result.fetchall()])
            
            # æ‰¾å‡ºç¼ºå¤±çš„è‚¡ç¥¨
            all_codes = set(codes_df['ts_code'].tolist())
            missing_codes = all_codes - existing_codes
            
            # æ£€æŸ¥BATCH_SIZEè®¾ç½®
            batch_size = os.getenv("BATCH_SIZE", "full")
            
            if missing_codes:
                logger.info(f"å‘ç° {len(missing_codes)} åªè‚¡ç¥¨ç¼ºå°‘å¸‚åœºæ•°æ®ï¼Œå°†ä¼˜å…ˆä¸‹è½½")
                missing_df = codes_df[codes_df['ts_code'].isin(missing_codes)]
                fetch_market_daily(missing_df['ts_code'])
            
            # æ ¹æ®BATCH_SIZEå†³å®šæ˜¯å¦æ›´æ–°å·²æœ‰è‚¡ç¥¨
            # å¦‚æœå·²ç»ä¸‹è½½äº†ç¼ºå¤±çš„è‚¡ç¥¨ï¼Œä¸”BATCH_SIZE=fullï¼Œåˆ™ä¸å†é‡å¤ä¸‹è½½
            # åªæœ‰åœ¨å¢é‡æ›´æ–°æ¨¡å¼ä¸‹æ‰æ›´æ–°å·²æœ‰è‚¡ç¥¨
            if batch_size.lower() in ["none", "null", "full"]:
                # å…¨é‡æ›´æ–°æ¨¡å¼ï¼šå¦‚æœè¿˜æœ‰ç¼ºå¤±çš„è‚¡ç¥¨ï¼Œç»§ç»­ä¸‹è½½ï¼›å¦åˆ™åªæ›´æ–°å·²æœ‰è‚¡ç¥¨çš„æœ€æ–°æ•°æ®ï¼ˆæœ€è¿‘30å¤©ï¼‰
                if missing_codes:
                    logger.info(f"ç»§ç»­ä¸‹è½½ç¼ºå¤±çš„è‚¡ç¥¨æ•°æ®ï¼ˆå…±{len(missing_codes)}åªï¼‰")
                else:
                    logger.info(f"æ‰€æœ‰è‚¡ç¥¨æ•°æ®å·²å®Œæ•´ï¼Œè·³è¿‡å…¨é‡æ›´æ–°")
                    # å¯ä»¥é€‰æ‹©æ€§åœ°æ›´æ–°æœ€è¿‘å‡ å¤©çš„æ•°æ®ï¼Œä½†è¿™é‡Œå…ˆè·³è¿‡ï¼Œé¿å…é‡å¤ä¸‹è½½
            else:
                # æ‰¹é‡æ›´æ–°ï¼šåªæ›´æ–°æŒ‡å®šæ•°é‡çš„è‚¡ç¥¨
                try:
                    limit = int(batch_size)
                    # ä¼˜å…ˆæ›´æ–°ç¼ºå¤±çš„è‚¡ç¥¨ï¼Œå¦‚æœè¿˜æœ‰å‰©ä½™ï¼Œå†æ›´æ–°å·²æœ‰è‚¡ç¥¨
                    if len(missing_codes) < limit:
                        remaining = limit - len(missing_codes)
                        existing_update_codes = codes_df[~codes_df['ts_code'].isin(missing_codes)].head(remaining)
                        if len(existing_update_codes) > 0:
                            logger.info(f"æ›´æ–° {remaining} åªå·²æœ‰è‚¡ç¥¨çš„æœ€æ–°æ•°æ®")
                            fetch_market_daily(existing_update_codes['ts_code'])
                    logger.info(f"æ‰¹é‡æ›´æ–°å®Œæˆï¼ˆå…±æ›´æ–° {min(limit, len(codes_df))} åªè‚¡ç¥¨ï¼‰")
                except ValueError:
                    logger.warning(f"BATCH_SIZEå€¼æ— æ•ˆ: {batch_size}ï¼Œè·³è¿‡å·²æœ‰è‚¡ç¥¨æ›´æ–°")
    except Exception as e:
        logger.warning(f"æ£€æŸ¥å·²æœ‰æ•°æ®æ—¶å‡ºé”™: {e}ï¼Œå°†å…¨é‡ä¸‹è½½")
        batch_size = os.getenv("BATCH_SIZE", "full")
        if batch_size.lower() in ["none", "null", "full"]:
            fetch_market_daily(codes_df['ts_code'])
        else:
            try:
                limit = int(batch_size)
                fetch_market_daily(codes_df.head(limit)['ts_code'])
            except ValueError:
                fetch_market_daily(codes_df['ts_code'])
    
    logger.info("âœ… å…¨éƒ¨å®Œæˆ")


if __name__ == "__main__":
    main()
