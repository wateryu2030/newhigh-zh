"""
BaoStockæ•°æ®ä¸‹è½½æ¨¡å—
åªä½¿ç”¨BaoStockä½œä¸ºå”¯ä¸€æ•°æ®æºï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
"""
import os
import sys
import time
from datetime import datetime, timedelta
import pandas as pd
from pathlib import Path

# Add data_engine directory to path for imports
data_engine_dir = Path(__file__).parent
sys.path.insert(0, str(data_engine_dir))

from utils.logger import setup_logger
from utils.retry import retry
from utils.db_utils import get_engine, upsert_df

from config import DB_URL, START_DATE, END_DATE, SLEEP_SEC_WEB

import baostock as bs

logger = setup_logger(log_file=os.path.join(os.path.dirname(__file__), "data_cache/update.log"))
engine = get_engine(DB_URL)


# ------------------ åŸºç¡€ä¿¡æ¯ ------------------
@retry(tries=5, delay=1.0)
def fetch_stock_basic_info():
    """è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ï¼Œåªä¿ç•™type=1çš„è‚¡ç¥¨ï¼Œæ’é™¤æŒ‡æ•°"""
    logger.info("å¼€å§‹è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯...")
    lg = bs.login()
    if lg.error_code != '0':
        raise RuntimeError(f"baostock ç™»å½•å¤±è´¥: {lg.error_msg}")
    
    rs = bs.query_stock_basic()
    rows = []
    while rs.error_code == '0' and rs.next():
        rows.append(rs.get_row_data())
    
    if not rows:
        bs.logout()
        raise RuntimeError("æœªè·å–åˆ°è‚¡ç¥¨åŸºç¡€ä¿¡æ¯")
    
    df = pd.DataFrame(rows, columns=rs.fields)
    bs.logout()
    
    # è¿‡æ»¤ï¼šåªä¿ç•™è‚¡ç¥¨ï¼ˆtype=1ï¼‰ï¼Œæ’é™¤æŒ‡æ•°ï¼ˆtype=2ï¼‰
    if 'type' in df.columns:
        df = df[df['type'] == '1']
        logger.info(f"è¿‡æ»¤åå‰©ä½™è‚¡ç¥¨: {len(df)} åªï¼ˆæ’é™¤äº†æŒ‡æ•°ï¼‰")
    
    # baostockçš„codeè½¬ts_codeé£æ ¼
    def to_ts(code):
        p = str(code).split('.')
        return (p[1] + ('.SH' if p[0]=='sh' else '.SZ')) if len(p)==2 else code
    
    df['ts_code'] = df['code'].map(to_ts)
    
    # å­—æ®µæ˜ å°„
    column_mapping = {
        'code_name': 'name',
        'ipoDate': 'list_date',
        'outDate': 'delist_date'
    }
    df = df.rename(columns=column_mapping)
    
    # åˆ é™¤ä¸éœ€è¦çš„åˆ—
    for col in ['code', 'type', 'status']:
        if col in df.columns:
            df = df.drop(columns=[col])
    
    # æ·»åŠ ç¼ºå¤±åˆ—
    for col in ['symbol', 'area', 'industry', 'market', 'is_hs']:
        if col not in df.columns:
            df[col] = None
    
    # ç¡®ä¿åˆ—é¡ºåº
    expected_fields = ['ts_code', 'symbol', 'name', 'area', 'industry', 'market', 'list_date', 'delist_date', 'is_hs']
    for field in expected_fields:
        if field not in df.columns:
            df[field] = None
    df = df[expected_fields]
    
    df = df.drop_duplicates(subset=['ts_code'])
    # ä½¿ç”¨upserté€»è¾‘ï¼Œæ”¯æŒå¢é‡æ›´æ–°
    upsert_df(df, "stock_basic_info", engine, if_exists="append")
    logger.info(f"stock_basic_info å†™å…¥ {len(df)} æ¡")
    return df[['ts_code']].dropna()


# ------------------ æ—¥è¡Œæƒ…ï¼ˆKçº¿+è´¢åŠ¡æŒ‡æ ‡ï¼‰ ------------------
@retry(tries=5, delay=1.0)
def fetch_market_daily(ts_codes: pd.Series):
    """è·å–æ—¥Kçº¿æ•°æ®ï¼ŒåŒ…å«PE/PB/PSç­‰è´¢åŠ¡æŒ‡æ ‡"""
    logger.info(f"å¼€å§‹ä¸‹è½½æ—¥Kè¡Œæƒ…æ•°æ®ï¼Œå…±{len(ts_codes)}åªè‚¡ç¥¨")
    lg = bs.login()
    if lg.error_code != '0':
        raise RuntimeError(f"baostock ç™»å½•å¤±è´¥: {lg.error_msg}")
    
    # BaoStockæ ‡å‡†å­—æ®µåˆ—è¡¨
    fields = "date,code,open,high,low,close,preclose,volume,amount,turn,pctChg,peTTM,pbMRQ,psTTM"
    
    total = 0
    success_count = 0
    failed_count = 0
    failed_codes = []
    
    for i, row in ts_codes.reset_index(drop=True).iterrows():
        ts_code = row['ts_code']
        # è½¬baostockä»£ç é£æ ¼
        if ts_code.endswith(".SH"):
            code = "sh." + ts_code.split('.')[0]
        else:
            code = "sz." + ts_code.split('.')[0]
        
        try:
            # è·å–Kçº¿æ•°æ®å’Œè´¢åŠ¡æŒ‡æ ‡
            rs = bs.query_history_k_data_plus(
                code,
                fields,
                start_date=START_DATE,
                end_date=END_DATE,
                frequency="d",
                adjustflag="3"
            )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if rs.error_code != '0':
                logger.warning(f"  âš ï¸ {ts_code} ({code}) æŸ¥è¯¢å¤±è´¥: {rs.error_msg}")
                failed_count += 1
                failed_codes.append(ts_code)
                time.sleep(SLEEP_SEC_WEB)
                continue
            
            rows = []
            while rs.error_code == '0' and rs.next():
                rows.append(rs.get_row_data())
            
            if rows:
                df = pd.DataFrame(rows, columns=rs.fields)
                # æ ‡å‡†åˆ—åæ˜ å°„
                df.rename(columns={
                    "date": "trade_date",
                    "pctChg": "pct_chg",
                    "turn": "turnover_rate"
                }, inplace=True)
                df["ts_code"] = ts_code
                
                # åˆ é™¤codeåˆ—
                if 'code' in df.columns:
                    df = df.drop(columns=['code'])
                
                # ç¡®ä¿æ‰€æœ‰æ•°å€¼åˆ—ä¸ºæ•°å€¼ç±»å‹
                numeric_cols = ['open', 'high', 'low', 'close', 'preclose', 'pct_chg', 'volume', 
                              'amount', 'turnover_rate', 'peTTM', 'pbMRQ', 'psTTM']
                for col in numeric_cols:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                
                # è®¡ç®—æŒ¯å¹…
                df['amplitude'] = ((df['high'] - df['low']) / df['preclose'] * 100).round(2)
                
                # ç¡®ä¿åˆ—é¡ºåºï¼ˆåŒ¹é…æ•°æ®åº“ï¼‰
                expected_fields = ['ts_code', 'trade_date', 'open', 'high', 'low', 'close', 'preclose',
                                 'pct_chg', 'volume', 'amount', 'turnover_rate', 'amplitude', 
                                 'peTTM', 'pbMRQ', 'psTTM']
                for field in expected_fields:
                    if field not in df.columns:
                        df[field] = None
                df = df[expected_fields]
                
                upsert_df(df, "stock_market_daily", engine, if_exists="append")
                total += len(df)
                
                # åŒæ—¶å°†PE/PB/PSæ•°æ®å†™å…¥è´¢åŠ¡è¡¨
                if 'peTTM' in df.columns or 'pbMRQ' in df.columns:
                    df_fin = df[['ts_code', 'trade_date']].copy()
                    if 'peTTM' in df.columns:
                        df_fin['pe'] = df['peTTM']
                    if 'pbMRQ' in df.columns:
                        df_fin['pb'] = df['pbMRQ']
                    if 'psTTM' in df.columns:
                        df_fin['ps'] = df['psTTM']
                    
                    # è¡¥å……å…¶ä»–å­—æ®µ
                    for col in ['pcf', 'roe', 'roa', 'eps', 'bps', 'total_mv', 'circ_mv', 
                              'revenue_yoy', 'net_profit_yoy', 'gross_profit_margin']:
                        df_fin[col] = None
                    
                    df_fin = df_fin.dropna(subset=['pe', 'pb'], how='all')
                    if not df_fin.empty:
                        upsert_df(df_fin, "stock_financials", engine, if_exists="append")
                success_count += 1
            else:
                logger.warning(f"  âš ï¸ {ts_code} ({code}) æ— æ•°æ®è¿”å›")
                failed_count += 1
                failed_codes.append(ts_code)
        except Exception as e:
            logger.error(f"  âŒ {ts_code} ({code}) ä¸‹è½½å‡ºé”™: {e}")
            failed_count += 1
            failed_codes.append(ts_code)
        
        if (i+1) % 50 == 0:
            logger.info(f"æ—¥Kè¿›åº¦ {i+1}/{len(ts_codes)} | æˆåŠŸ: {success_count} | å¤±è´¥: {failed_count}")
        
        time.sleep(SLEEP_SEC_WEB)
    
    bs.logout()
    logger.info(f"stock_market_daily å…±å†™å…¥ {total} è¡Œ")
    logger.info(f"âœ… ä¸‹è½½å®Œæˆ: æˆåŠŸ {success_count} åªï¼Œå¤±è´¥ {failed_count} åª")
    if failed_codes:
        logger.warning(f"å¤±è´¥çš„è‚¡ç¥¨ä»£ç ï¼ˆå‰20ä¸ªï¼‰: {failed_codes[:20]}")
    return total


def main():
    """ä¸»å‡½æ•°ï¼šä¸‹è½½æ‰€æœ‰æ•°æ®"""
    logger.info("ğŸš€ å¼€å§‹æ›´æ–° Aè‚¡æ™ºèƒ½é€‰è‚¡åŸºç¡€æ•°æ®åº“")
    logger.info(f"æŠ“å–çª—å£: {START_DATE} ~ {END_DATE}")
    
    # 1. è·å–åŸºç¡€ä¿¡æ¯ï¼ˆé™æ€æ•°æ®ï¼Œå…¨é‡æ›´æ–°ï¼‰
    codes_df = fetch_stock_basic_info()
    
    # 2. ä¸‹è½½æ—¥Kè¡Œæƒ…ï¼ˆæ™ºèƒ½å¢é‡æ›´æ–°ï¼šä¼˜å…ˆä¸‹è½½ç¼ºå¤±çš„æ•°æ®ï¼‰
    # æ£€æŸ¥æ•°æ®åº“ä¸­å·²æœ‰çš„è‚¡ç¥¨ï¼Œåªä¸‹è½½ç¼ºå¤±çš„æ•°æ®
    try:
        import sqlite3
        from pathlib import Path
        db_path = Path(__file__).parent.parent / "data" / "stock_database.db"
        if db_path.exists():
            # ä½¿ç”¨è¶…æ—¶è¿æ¥ï¼Œé¿å…æ•°æ®åº“é”å®š
            conn = sqlite3.connect(str(db_path), timeout=30.0)
            cursor = conn.cursor()
            # è·å–å·²æœ‰æœ€æ–°æ—¥æœŸçš„è‚¡ç¥¨åˆ—è¡¨
            cursor.execute("""
                SELECT DISTINCT ts_code 
                FROM stock_market_daily 
                WHERE trade_date = (SELECT MAX(trade_date) FROM stock_market_daily)
            """)
            existing_codes = set([row[0] for row in cursor.fetchall()])
            conn.close()
            
            # æ‰¾å‡ºç¼ºå¤±çš„è‚¡ç¥¨
            all_codes = set(codes_df['ts_code'].tolist())
            missing_codes = all_codes - existing_codes
            
            # æ£€æŸ¥BATCH_SIZEè®¾ç½®
            batch_size = os.getenv("BATCH_SIZE", "full")
            
            if missing_codes:
                logger.info(f"å‘ç° {len(missing_codes)} åªè‚¡ç¥¨ç¼ºå°‘å¸‚åœºæ•°æ®ï¼Œå°†ä¼˜å…ˆä¸‹è½½")
                missing_df = codes_df[codes_df['ts_code'].isin(missing_codes)]
                fetch_market_daily(missing_df)
            
            # æ ¹æ®BATCH_SIZEå†³å®šæ˜¯å¦æ›´æ–°å·²æœ‰è‚¡ç¥¨
            # å¦‚æœå·²ç»ä¸‹è½½äº†ç¼ºå¤±çš„è‚¡ç¥¨ï¼Œä¸”BATCH_SIZE=fullï¼Œåˆ™ä¸å†é‡å¤ä¸‹è½½
            # åªæœ‰åœ¨å¢é‡æ›´æ–°æ¨¡å¼ä¸‹æ‰æ›´æ–°å·²æœ‰è‚¡ç¥¨
            if batch_size.lower() in ["none", "null", "full"]:
                # å…¨é‡æ›´æ–°æ¨¡å¼ï¼šå¦‚æœè¿˜æœ‰ç¼ºå¤±çš„è‚¡ç¥¨ï¼Œç»§ç»­ä¸‹è½½ï¼›å¦åˆ™åªæ›´æ–°å·²æœ‰è‚¡ç¥¨çš„æœ€æ–°æ•°æ®ï¼ˆæœ€è¿‘30å¤©ï¼‰
                if missing_codes:
                    logger.info(f"ç»§ç»­ä¸‹è½½ç¼ºå¤±çš„è‚¡ç¥¨æ•°æ®ï¼ˆå…±{len(missing_codes)}åªï¼‰")
                else:
                    logger.info(f"æ‰€æœ‰è‚¡ç¥¨æ•°æ®å·²å®Œæ•´ï¼Œè·³è¿‡å…¨é‡æ›´æ–°")
                    # å¯ä»¥é€‰æ‹©æ€§åœ°æ›´æ–°æœ€è¿‘å‡ å¤©çš„æ•°æ®ï¼Œä½†è¿™é‡Œå…ˆè·³è¿‡ï¼Œé¿å…é‡å¤ä¸‹è½½
            else:
                # æ‰¹é‡æ›´æ–°ï¼šåªæ›´æ–°æŒ‡å®šæ•°é‡çš„è‚¡ç¥¨
                try:
                    limit = int(batch_size)
                    # ä¼˜å…ˆæ›´æ–°ç¼ºå¤±çš„è‚¡ç¥¨ï¼Œå¦‚æœè¿˜æœ‰å‰©ä½™ï¼Œå†æ›´æ–°å·²æœ‰è‚¡ç¥¨
                    if len(missing_codes) < limit:
                        remaining = limit - len(missing_codes)
                        existing_update_codes = codes_df[~codes_df['ts_code'].isin(missing_codes)].head(remaining)
                        if len(existing_update_codes) > 0:
                            logger.info(f"æ›´æ–° {remaining} åªå·²æœ‰è‚¡ç¥¨çš„æœ€æ–°æ•°æ®")
                            fetch_market_daily(existing_update_codes)
                    logger.info(f"æ‰¹é‡æ›´æ–°å®Œæˆï¼ˆå…±æ›´æ–° {min(limit, len(codes_df))} åªè‚¡ç¥¨ï¼‰")
                except ValueError:
                    logger.warning(f"BATCH_SIZEå€¼æ— æ•ˆ: {batch_size}ï¼Œè·³è¿‡å·²æœ‰è‚¡ç¥¨æ›´æ–°")
        else:
            # æ•°æ®åº“ä¸å­˜åœ¨ï¼Œå…¨é‡ä¸‹è½½
            logger.info(f"æ•°æ®åº“ä¸å­˜åœ¨ï¼Œå…¨é‡ä¸‹è½½æ‰€æœ‰è‚¡ç¥¨æ—¥Kæ•°æ®ï¼ˆå…±{len(codes_df)}åªï¼‰")
            fetch_market_daily(codes_df)
    except Exception as e:
        logger.warning(f"æ£€æŸ¥å·²æœ‰æ•°æ®æ—¶å‡ºé”™: {e}ï¼Œå°†å…¨é‡ä¸‹è½½")
        batch_size = os.getenv("BATCH_SIZE", "full")
        if batch_size.lower() in ["none", "null", "full"]:
            fetch_market_daily(codes_df)
        else:
            try:
                limit = int(batch_size)
                fetch_market_daily(codes_df.head(limit))
            except ValueError:
                fetch_market_daily(codes_df)
    
    logger.info("âœ… å…¨éƒ¨å®Œæˆ")


if __name__ == "__main__":
    main()
